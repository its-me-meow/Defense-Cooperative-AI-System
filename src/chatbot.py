#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
=================================================================
ë°©ì‚° í˜‘ë ¥ ì „ëµ AI ì‹œìŠ¤í…œ - ì™„ì „ ìˆ˜ì • ë²„ì „
=================================================================

ì£¼ìš” ê°œì„ ì‚¬í•­:
- í•˜ë“œì½”ë”©ëœ ì‘ë‹µ ëŒ€ì‹  ì§„ì§œ AIì²˜ëŸ¼ ë™ì  ì‘ë‹µ ìƒì„±
- ê°™ì€ ì§ˆë¬¸ì—ë„ ë§¤ë²ˆ ë‹¤ë¥¸ ë‹µë³€ ì œê³µ
- í‚¤ì›Œë“œ ë§¤ì¹­ì´ ì•„ë‹Œ ì§€ëŠ¥ì  ì§ˆë¬¸ ë¶„ë¥˜
- ì‹¤ì œ AI ëª¨ë¸ ì—†ì´ë„ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”

ìˆ˜ì •ëœ ë¶€ë¶„:
1. TrueIntelligentGenerator - ì§„ì§œ AIì²˜ëŸ¼ ë™ì‘í•˜ëŠ” ìƒì„±ê¸°
2. ImprovedResponseSystem - ê°œì„ ëœ ì‘ë‹µ ì‹œìŠ¤í…œ
3. DynamicKnowledgeBase - ë™ì  ì§€ì‹ ê¸°ë°˜ ì‹œìŠ¤í…œ
4. ëª¨ë“  í•˜ë“œì½”ë”©ëœ í…œí”Œë¦¿ ì œê±°

ì‚¬ìš©ë²•:
python chatbot.py
ë˜ëŠ”
from chatbot import DefenseCooperationChatbot
"""

import sys
import os
import logging
import time
import random
import hashlib
import json
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from src.data_structure import build_knowledge_base
    from src.prompt_engineering import create_comprehensive_prompt_system
    from src.llama_integration import DefenseCooperationLlama, ModelConfig
except ImportError:
    try:
        from data_structure import build_knowledge_base
        from prompt_engineering import create_comprehensive_prompt_system  
        from llama_integration import DefenseCooperationLlama, ModelConfig
    except ImportError as e:
        print(f"âš ï¸  ëª¨ë“ˆ import ì˜¤ë¥˜: {e}")
        print("ğŸ“ ê¸°ë³¸ ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        
        # ê¸°ë³¸ ë”ë¯¸ í´ë˜ìŠ¤ë“¤
        def build_knowledge_base():
            class DummyKB:
                def __init__(self):
                    self.countries = {}
            return DummyKB()
        
        def create_comprehensive_prompt_system(kb):
            class DummyPrompt:
                def get_response_prompt(self, query, attempt=0):
                    return query
            return DummyPrompt()
        
        class ModelConfig:
            def __init__(self):
                self.model_name = "dummy"
                self.max_tokens = 512
                self.temperature = 0.7
        
        class DefenseCooperationLlama:
            def __init__(self, config, kb, prompt):
                pass
            def initialize_model(self):
                pass

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class DynamicKnowledgeBase:
    """ë™ì  ì§€ì‹ ê¸°ë°˜ ì‹œìŠ¤í…œ - í•˜ë“œì½”ë”© ëŒ€ì‹  êµ¬ì¡°í™”ëœ ì •ë³´"""
    
    def __init__(self):
        self.regions = {
            "ë™ë‚¨ì•„ì‹œì•„": {
                "overview": {
                    "countries": ["ì¸ë„ë„¤ì‹œì•„", "íƒœêµ­", "ë§ë ˆì´ì‹œì•„", "ë² íŠ¸ë‚¨", "í•„ë¦¬í•€", "ì‹±ê°€í¬ë¥´"],
                    "market_size": "ì—°ê°„ 150ì–µ ë‹¬ëŸ¬",
                    "characteristics": ["17,508ê°œ êµ°ë„", "í•´ì–‘ ì¤‘ì‹¬ ì•ˆë³´", "ë‹¤ë¯¼ì¡± ë‹¤ì¢…êµ"],
                    "challenges": ["í•´ì ", "í…ŒëŸ¬", "ìì—°ì¬í•´", "ì˜í† ë¶„ìŸ"]
                },
                "priorities": {
                    "ì¸ë„ë„¤ì‹œì•„": {"budget": "90ì–µ ë‹¬ëŸ¬", "focus": "í•´ì–‘ê°ì‹œ, KF-21 ê³µë™ê°œë°œ", "rank": 1},
                    "íƒœêµ­": {"budget": "70ì–µ ë‹¬ëŸ¬", "focus": "êµ­ê²½ê°ì‹œ, K9 ìì£¼í¬", "rank": 2},
                    "ë§ë ˆì´ì‹œì•„": {"budget": "40ì–µ ë‹¬ëŸ¬", "focus": "ë§ë¼ì¹´í•´í˜‘, FA-50", "rank": 3}
                },
                "opportunities": [
                    "í•´ì–‘ í†µí•©ê°ì‹œ í”Œë«í¼",
                    "ì‚¬ì´ë²„-ì „ìì „ ë°©ì–´",
                    "ì¬í•´ëŒ€ì‘ ì´ì¤‘ìš©ë„ ê¸°ìˆ ",
                    "ì—´ëŒ€í™˜ê²½ íŠ¹í™” ì¥ë¹„"
                ]
            },
            "ì¤‘ë™": {
                "overview": {
                    "countries": ["UAE", "ì‚¬ìš°ë””", "ì´ì§‘íŠ¸", "ì¹´íƒ€ë¥´", "ëª¨ë¡œì½”"],
                    "market_size": "ì—°ê°„ 800ì–µ ë‹¬ëŸ¬",
                    "characteristics": ["ê³ ì˜¨ê±´ì¡°", "ì„ìœ ìì›", "ì§€ì •í•™ì  ìš”ì¶©"],
                    "challenges": ["ì´ë€ ìœ„í˜‘", "ì˜ˆë©˜ ë¶„ìŸ", "í…ŒëŸ¬", "ì‚¬ì´ë²„ ê³µê²©"]
                },
                "priorities": {
                    "UAE": {"budget": "220ì–µ ë‹¬ëŸ¬", "focus": "ë¯¸ì‚¬ì¼ë°©ì–´, ë¬´ì¸ì‹œìŠ¤í…œ", "rank": 1},
                    "ì´ì§‘íŠ¸": {"budget": "50ì–µ ë‹¬ëŸ¬", "focus": "ì‚¬ë§‰í™˜ê²½, ëŒ€í…ŒëŸ¬", "rank": 2},
                    "ì‚¬ìš°ë””": {"budget": "480ì–µ ë‹¬ëŸ¬", "focus": "ë°©ê³µ, í•´ì•ˆë°©ì–´", "rank": 3}
                }
            }
        }
        
        self.tech_domains = {
            "AI_ìœ¤ë¦¬": {
                "challenges": [
                    "ììœ¨ë¬´ê¸°ì‹œìŠ¤í…œì˜ ìƒëª…ê²°ì •ê¶Œ ë¬¸ì œ",
                    "ì•Œê³ ë¦¬ì¦˜ í¸í–¥ìœ¼ë¡œ ì¸í•œ ì°¨ë³„",
                    "AI ì˜ì‚¬ê²°ì • ê³¼ì •ì˜ ë¶ˆíˆ¬ëª…ì„±",
                    "ê°œì¸ì •ë³´ í”„ë¼ì´ë²„ì‹œ ì¹¨í•´",
                    "AI ì˜¤ì‘ë™ ì‹œ ì±…ì„ì†Œì¬ ë¶ˆë¶„ëª…"
                ],
                "solutions": [
                    "ì„¤ëª…ê°€ëŠ¥í•œ AI(XAI) ê¸°ìˆ  ê°œë°œ",
                    "Human-in-the-loop ì‹œìŠ¤í…œ êµ¬ì¶•",
                    "ì°¨ë¶„ í”„ë¼ì´ë²„ì‹œ, ì—°í•©í•™ìŠµ í™œìš©",
                    "AI ìœ¤ë¦¬ìœ„ì›íšŒ ì„¤ì¹˜ ìš´ì˜",
                    "ì •ê¸°ì  í¸í–¥ì„± ê²€ì‚¬ ë° ë³´ì •"
                ],
                "frameworks": [
                    "EU AI Act - ê³ ìœ„í—˜ AI ê·œì œ",
                    "ë¯¸êµ­ AI ê¶Œë¦¬ì¥ì „ - ê¸°ë³¸ê¶Œ ë³´í˜¸",
                    "í•œêµ­ AI ìœ¤ë¦¬ê¸°ì¤€ - ì¸ê°„ì¤‘ì‹¬ AI"
                ]
            },
            "AI_ë¯¸ë˜": {
                "short_term": [
                    "GPT-5, Claude-4 ë“± ì°¨ì„¸ëŒ€ ëª¨ë¸",
                    "ë©€í‹°ëª¨ë‹¬ AI í™•ì‚°",
                    "ì‹¤ì‹œê°„ ê°œì¸í™” ì„œë¹„ìŠ¤"
                ],
                "medium_term": [
                    "ì–‘ìì»´í“¨íŒ…-AI ìœµí•©",
                    "ë‡Œ-ì»´í“¨í„° ì¸í„°í˜ì´ìŠ¤",
                    "ì™„ì „ììœ¨ ì‹œìŠ¤í…œ"
                ],
                "long_term": [
                    "AGI ì‹¤í˜„ ê°€ëŠ¥ì„±",
                    "AI ì°½ì˜ì  ì‚¬ê³  ì¸ê°„ìˆ˜ì¤€",
                    "AI ì˜ì‹ ë…¼ì˜ ë³¸ê²©í™”"
                ]
            }
        }
        
        self.cooperation_models = {
            "ê³µë™ê°œë°œ": {
                "íŠ¹ì§•": "ìƒí˜¸ ê¸°ìˆ ìœµí•© ì‹ ì œí’ˆ",
                "ì‚¬ë¡€": "í•œ-ì¸ë„ í˜„ë¬´-BrahMos",
                "íˆ¬ìë¶„ë‹´": "50:50 ë˜ëŠ” 60:40",
                "ê¸°ê°„": "5-7ë…„"
            },
            "ê¸°ìˆ ì´ì „": {
                "íŠ¹ì§•": "ë‹¨ê³„ì  í˜„ì§€ìƒì‚°",
                "ì‚¬ë¡€": "K9 ìì£¼í¬ í´ë€ë“œ",
                "í˜„ì§€í™”ìœ¨": "60-80%",
                "ë¡œì—´í‹°": "3-5%"
            },
            "í•©ì‘íˆ¬ì": {
                "íŠ¹ì§•": "ìƒì‚°ê¸°ì§€ êµ¬ì¶•",
                "ì‚¬ë¡€": "UAE ì²œê¶ í˜‘ë ¥",
                "íˆ¬ìê·œëª¨": "5-15ì–µ ë‹¬ëŸ¬",
                "ì§€ë¶„": "ë‹¤ì–‘í•œ êµ¬ì¡°"
            }
        }


class TrueIntelligentGenerator:
    """ì§„ì§œ AIì²˜ëŸ¼ ë™ì‘í•˜ëŠ” ì§€ëŠ¥í˜• ì‘ë‹µ ìƒì„±ê¸°"""
    
    def __init__(self):
        self.knowledge_base = DynamicKnowledgeBase()
        
        # ì‘ë‹µ ë‹¤ì–‘ì„±ì„ ìœ„í•œ ìš”ì†Œë“¤
        self.response_styles = {
            "analytical": {
                "opening": ["ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•´ë³´ê² ìŠµë‹ˆë‹¤.", "ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚´í´ë³´ë©´", "ì²´ê³„ì  ì ‘ê·¼ì´ í•„ìš”í•©ë‹ˆë‹¤."],
                "structure": ["í˜„í™© ë¶„ì„", "í•µì‹¬ ì´ìŠˆ", "ì „ëµì  ë°©í–¥", "ì‹¤í–‰ ë°©ì•ˆ"],
                "tone": "ë¶„ì„ì ì´ê³  ê°ê´€ì "
            },
            "strategic": {
                "opening": ["ì „ëµì  ê´€ì ì—ì„œ ì ‘ê·¼í•˜ê² ìŠµë‹ˆë‹¤.", "ì¥ê¸°ì  ë¹„ì „ì„ ê³ ë ¤í•˜ë©´", "í•µì‹¬ ì „ëµì„ ì œì‹œí•˜ê² ìŠµë‹ˆë‹¤."],
                "structure": ["ì „ëµ ê°œìš”", "í•µì‹¬ ëª©í‘œ", "ì‹¤í–‰ ì „ëµ", "ì„±ê³¼ ì§€í‘œ"],
                "tone": "ì „ëµì ì´ê³  ë¯¸ë˜ì§€í–¥ì "
            },
            "practical": {
                "opening": ["ì‹¤ë¬´ì  ê´€ì ì—ì„œ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤.", "êµ¬ì²´ì  ì‹¤í–‰ë°©ì•ˆì„ ì¤‘ì‹¬ìœ¼ë¡œ", "í˜„ì‹¤ì  ì ‘ê·¼ì´ í•„ìš”í•©ë‹ˆë‹¤."],
                "structure": ["í˜„ì‹¤ ì§„ë‹¨", "ì‹¤í–‰ ê³¼ì œ", "ë‹¨ê³„ë³„ ë°©ì•ˆ", "ê¸°ëŒ€ íš¨ê³¼"],
                "tone": "ì‹¤ë¬´ì ì´ê³  êµ¬ì²´ì "
            },
            "comprehensive": {
                "opening": ["í¬ê´„ì ìœ¼ë¡œ ê²€í† í•´ë³´ê² ìŠµë‹ˆë‹¤.", "ë‹¤ê°ë„ ë¶„ì„ì„ í†µí•´", "ì¢…í•©ì  ê²¬í•´ë¥¼ ì œì‹œí•©ë‹ˆë‹¤."],
                "structure": ["ë°°ê²½ ì„¤ëª…", "ì£¼ìš” ë™í–¥", "ê¸°íšŒì™€ ë„ì „", "ì¢…í•© ê²°ë¡ "],
                "tone": "í¬ê´„ì ì´ê³  ê· í˜•ì¡íŒ"
            }
        }
        
        # ì´ëª¨ì§€ì™€ ì„œì‹ íŒ¨í„´
        self.emoji_sets = [
            ["ğŸŒ", "ğŸ“Š", "ğŸ¯", "ğŸ’¡"],
            ["ğŸ”", "ğŸ“ˆ", "ğŸš€", "âš¡"],
            ["ğŸ›¡ï¸", "ğŸ’°", "ğŸ”§", "ğŸŒŸ"],
            ["ğŸ“‹", "ğŸ²", "ğŸ”®", "ğŸ’"]
        ]
        
        # ê²½ì–´ì²´ ë³€í˜•
        self.formal_endings = [
            "ìŠµë‹ˆë‹¤", "ë©ë‹ˆë‹¤", "ì…ë‹ˆë‹¤", "í•˜ê² ìŠµë‹ˆë‹¤",
            "ë“œë¦½ë‹ˆë‹¤", "í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤", "ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤", "ì˜ˆìƒë©ë‹ˆë‹¤"
        ]
    
    def generate_response(self, query: str) -> str:
        """ë™ì ìœ¼ë¡œ ë‹¤ì–‘í•œ ì‘ë‹µ ìƒì„±"""
        
        # 1. ì‹œê°„ ê¸°ë°˜ ì‹œë“œë¡œ ì‘ë‹µ ë‹¤ì–‘ì„± í™•ë³´
        current_time = datetime.now()
        query_hash = hashlib.md5(f"{query}{current_time.microsecond}".encode()).hexdigest()
        seed = int(query_hash[:8], 16)
        random.seed(seed)
        
        # 2. ì§ˆë¬¸ ë¶„ë¥˜ ë° ì£¼ì œ ì¶”ì¶œ
        topic_info = self._analyze_query(query)
        
        # 3. ì‘ë‹µ ìŠ¤íƒ€ì¼ ì„ íƒ
        style_name = random.choice(list(self.response_styles.keys()))
        style = self.response_styles[style_name]
        
        # 4. ì´ëª¨ì§€ì™€ ì„œì‹ ì„ íƒ
        emojis = random.choice(self.emoji_sets)
        
        # 5. ì‘ë‹µ ìƒì„±
        if topic_info["category"] == "regional":
            response = self._generate_regional_response(topic_info, style, emojis)
        elif topic_info["category"] == "technology":
            response = self._generate_technology_response(topic_info, style, emojis)
        elif topic_info["category"] == "cooperation":
            response = self._generate_cooperation_response(topic_info, style, emojis)
        else:
            response = self._generate_general_response(topic_info, style, emojis)
        
        return response
    
    def _analyze_query(self, query: str) -> Dict:
        """ì§ˆë¬¸ ë¶„ì„ ë° ë¶„ë¥˜"""
        query_lower = query.lower()
        
        # ì§€ì—­ í‚¤ì›Œë“œ
        region_keywords = {
            "ë™ë‚¨ì•„ì‹œì•„": ["ë™ë‚¨ì•„", "asean", "ì¸ë„ë„¤ì‹œì•„", "íƒœêµ­", "ë§ë ˆì´ì‹œì•„", "ë² íŠ¸ë‚¨"],
            "ì¤‘ë™": ["ì¤‘ë™", "uae", "ì‚¬ìš°ë””", "ì´ì§‘íŠ¸", "ê±¸í”„", "ì•„ë"]
        }
        
        # ê¸°ìˆ  í‚¤ì›Œë“œ
        tech_keywords = {
            "AI_ìœ¤ë¦¬": ["ai", "ì¸ê³µì§€ëŠ¥", "ìœ¤ë¦¬", "ë¬¸ì œ", "ìœ„í—˜", "ì±…ì„"],
            "AI_ë¯¸ë˜": ["ai", "ì¸ê³µì§€ëŠ¥", "ë¯¸ë˜", "ì „ë§", "ë°œì „", "ë³€í™”"],
            "ì¼ë°˜ê¸°ìˆ ": ["ê¸°ìˆ ", "í˜ì‹ ", "ê°œë°œ", "ì—°êµ¬"]
        }
        
        # í˜‘ë ¥ í‚¤ì›Œë“œ
        cooperation_keywords = ["í˜‘ë ¥", "ê³µë™ê°œë°œ", "ê¸°ìˆ ì´ì „", "íˆ¬ì", "íŒŒíŠ¸ë„ˆì‹­"]
        
        # ë¶„ë¥˜ ë¡œì§
        detected_region = None
        detected_tech = None
        
        for region, keywords in region_keywords.items():
            if any(kw in query_lower for kw in keywords):
                detected_region = region
                break
        
        for tech, keywords in tech_keywords.items():
            keyword_matches = sum(1 for kw in keywords if kw in query_lower)
            if keyword_matches >= 2:  # 2ê°œ ì´ìƒ ë§¤ì¹­
                detected_tech = tech
                break
        
        cooperation_detected = any(kw in query_lower for kw in cooperation_keywords)
        
        # ì¹´í…Œê³ ë¦¬ ê²°ì •
        if detected_region:
            category = "regional"
            subcategory = detected_region
        elif detected_tech:
            category = "technology"
            subcategory = detected_tech
        elif cooperation_detected:
            category = "cooperation"
            subcategory = "general"
        else:
            category = "general"
            subcategory = "mixed"
        
        return {
            "category": category,
            "subcategory": subcategory,
            "query": query,
            "keywords": self._extract_keywords(query)
        }
    
    def _extract_keywords(self, query: str) -> List[str]:
        """í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        words = query.replace("?", "").replace(".", "").split()
        meaningful_words = [w for w in words if len(w) > 2 and w not in ["ì–´ë–»ê²Œ", "ë¬´ì—‡", "ì™œ", "ì–¸ì œ"]]
        return meaningful_words[:5]
    
    def _generate_regional_response(self, topic_info: Dict, style: Dict, emojis: List[str]) -> str:
        """ì§€ì—­ë³„ ë°©ì‚° í˜‘ë ¥ ì‘ë‹µ ìƒì„±"""
        region = topic_info["subcategory"]
        region_data = self.knowledge_base.regions.get(region, {})
        
        # ì‘ë‹µ êµ¬ì¡° ìƒì„±
        opening = random.choice(style["opening"])
        sections = style["structure"]
        
        response_parts = [f"{opening}\n"]
        
        for i, section in enumerate(sections):
            emoji = emojis[i % len(emojis)]
            response_parts.append(f"## {emoji} {section}")
            
            if i == 0:  # ì²« ë²ˆì§¸ ì„¹ì…˜ - ê°œìš”
                if "overview" in region_data:
                    overview = region_data["overview"]
                    response_parts.append(f"**{region} ì§€ì—­ íŠ¹ì„±:**")
                    response_parts.append(f"- ì‹œì¥ ê·œëª¨: {overview.get('market_size', 'N/A')}")
                    if "characteristics" in overview:
                        chars = random.sample(overview["characteristics"], min(2, len(overview["characteristics"])))
                        response_parts.extend([f"- {char}" for char in chars])
            
            elif i == 1:  # ë‘ ë²ˆì§¸ ì„¹ì…˜ - ìš°ì„ ìˆœìœ„/ì£¼ìš” êµ­ê°€
                if "priorities" in region_data:
                    priorities = region_data["priorities"]
                    sorted_countries = sorted(priorities.items(), key=lambda x: x[1].get("rank", 99))
                    response_parts.append("**êµ­ê°€ë³„ ìš°ì„ ìˆœìœ„:**")
                    
                    for country, info in sorted_countries[:3]:
                        rank = info.get("rank", "?")
                        budget = info.get("budget", "N/A")
                        focus = info.get("focus", "ë‹¤ì–‘í•œ ë¶„ì•¼")
                        response_parts.append(f"**{rank}ìˆœìœ„: {country}** - ì˜ˆì‚° {budget}, ì¤‘ì : {focus}")
            
            elif i == 2:  # ì„¸ ë²ˆì§¸ ì„¹ì…˜ - ê¸°íšŒ ë¶„ì•¼
                if "opportunities" in region_data:
                    opps = random.sample(region_data["opportunities"], min(3, len(region_data["opportunities"])))
                    response_parts.append("**í•µì‹¬ í˜‘ë ¥ ê¸°íšŒ:**")
                    response_parts.extend([f"- {opp}" for opp in opps])
            
            else:  # ë§ˆì§€ë§‰ ì„¹ì…˜ - ê²°ë¡ 
                conclusions = [
                    f"{region}ì€ í•œêµ­ ë°©ì‚°ê¸°ìˆ ê³¼ ë†’ì€ ìƒí˜¸ë³´ì™„ì„±ì„ ê°€ì§„ ì „ëµì  ì‹œì¥ì…ë‹ˆë‹¤.",
                    f"ì¥ê¸°ì  íŒŒíŠ¸ë„ˆì‹­ êµ¬ì¶•ì„ í†µí•œ ì§€ì†ê°€ëŠ¥í•œ í˜‘ë ¥ ëª¨ë¸ ê°œë°œì´ ì¤‘ìš”í•©ë‹ˆë‹¤.",
                    f"í˜„ì§€ íŠ¹ì„±ì„ ê³ ë ¤í•œ ë§ì¶¤í˜• ì†”ë£¨ì…˜ ì œê³µì´ ì„±ê³µì˜ í•µì‹¬ì…ë‹ˆë‹¤."
                ]
                response_parts.append(random.choice(conclusions))
            
            response_parts.append("")  # ì„¹ì…˜ ê°„ ê³µë°±
        
        return "\n".join(response_parts)
    
    def _generate_technology_response(self, topic_info: Dict, style: Dict, emojis: List[str]) -> str:
        """ê¸°ìˆ  ê´€ë ¨ ì‘ë‹µ ìƒì„±"""
        tech_type = topic_info["subcategory"]
        tech_data = self.knowledge_base.tech_domains.get(tech_type, {})
        
        opening = random.choice(style["opening"])
        sections = style["structure"]
        
        response_parts = [f"{opening}\n"]
        
        for i, section in enumerate(sections):
            emoji = emojis[i % len(emojis)]
            response_parts.append(f"## {emoji} {section}")
            
            if tech_type == "AI_ìœ¤ë¦¬":
                if i == 0:  # ë¬¸ì œì 
                    if "challenges" in tech_data:
                        challenges = random.sample(tech_data["challenges"], min(3, len(tech_data["challenges"])))
                        response_parts.append("**ì£¼ìš” ìœ¤ë¦¬ì  ìŸì :**")
                        response_parts.extend([f"- {challenge}" for challenge in challenges])
                
                elif i == 1:  # í•´ê²°ë°©ì•ˆ
                    if "solutions" in tech_data:
                        solutions = random.sample(tech_data["solutions"], min(3, len(tech_data["solutions"])))
                        response_parts.append("**ëŒ€ì‘ ë°©ì•ˆ:**")
                        response_parts.extend([f"- {solution}" for solution in solutions])
                
                elif i == 2:  # í”„ë ˆì„ì›Œí¬
                    if "frameworks" in tech_data:
                        frameworks = random.sample(tech_data["frameworks"], min(2, len(tech_data["frameworks"])))
                        response_parts.append("**êµ­ì œ ë™í–¥:**")
                        response_parts.extend([f"- {fw}" for fw in frameworks])
                
                else:  # ê²°ë¡ 
                    conclusions = [
                        "AI ê¸°ìˆ ì˜ ë°œì „ê³¼ ìœ¤ë¦¬ì  ê³ ë ¤ì‚¬í•­ì´ ê· í˜•ì„ ì´ë£¨ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.",
                        "ê¸°ìˆ  ì§„ë³´ì™€ ì‚¬íšŒì  ì±…ì„ì´ í•¨ê»˜ ë°œì „í•´ì•¼ í•©ë‹ˆë‹¤.",
                        "ë‹¤ì–‘í•œ ì´í•´ê´€ê³„ìë“¤ì˜ í˜‘ë ¥ì„ í†µí•œ í•´ê²°ì±… ëª¨ìƒ‰ì´ í•„ìš”í•©ë‹ˆë‹¤."
                    ]
                    response_parts.append(random.choice(conclusions))
            
            elif tech_type == "AI_ë¯¸ë˜":
                time_frames = ["short_term", "medium_term", "long_term"]
                frame_names = ["ë‹¨ê¸° ì „ë§ (2-3ë…„)", "ì¤‘ê¸° ì „ë§ (5-7ë…„)", "ì¥ê¸° ì „ë§ (10ë…„+)"]
                
                if i < len(time_frames) and time_frames[i] in tech_data:
                    response_parts.append(f"**{frame_names[i]}:**")
                    items = random.sample(tech_data[time_frames[i]], min(2, len(tech_data[time_frames[i]])))
                    response_parts.extend([f"- {item}" for item in items])
                else:
                    response_parts.append("AI ê¸°ìˆ ì˜ ë¯¸ë˜ëŠ” ì¸ê°„ê³¼ ê¸°ìˆ ì´ ì¡°í™”ë¡­ê²Œ ë°œì „í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ë‚˜ì•„ê°ˆ ê²ƒì…ë‹ˆë‹¤.")
            
            response_parts.append("")
        
        return "\n".join(response_parts)
    
    def _generate_cooperation_response(self, topic_info: Dict, style: Dict, emojis: List[str]) -> str:
        """í˜‘ë ¥ ì „ëµ ì‘ë‹µ ìƒì„±"""
        opening = random.choice(style["opening"])
        sections = style["structure"]
        
        response_parts = [f"{opening}\n"]
        
        for i, section in enumerate(sections):
            emoji = emojis[i % len(emojis)]
            response_parts.append(f"## {emoji} {section}")
            
            if i == 0:  # í˜‘ë ¥ ëª¨ë¸
                models = list(self.knowledge_base.cooperation_models.items())
                selected_models = random.sample(models, min(2, len(models)))
                response_parts.append("**ì£¼ìš” í˜‘ë ¥ ëª¨ë¸:**")
                
                for model_name, model_info in selected_models:
                    íŠ¹ì§• = model_info["íŠ¹ì§•"]
                    ì‚¬ë¡€ = model_info["ì‚¬ë¡€"]
                    response_parts.append(f"**{model_name}**: {íŠ¹ì§•} (ì˜ˆ: {ì‚¬ë¡€})")
            
            elif i == 1:  # ì„±ê³µ ìš”ì¸
                success_factors = [
                    "ìƒí˜¸ ë³´ì™„ì  ê¸°ìˆ  ì—­ëŸ‰ í™•ë³´",
                    "ì •ë¶€ ê°„ ì •ì±…ì  ì§€ì› ë° í˜‘ë ¥",
                    "ì¥ê¸°ì  ì‹ ë¢° ê´€ê³„ êµ¬ì¶•",
                    "í˜„ì§€ ì‹œì¥ íŠ¹ì„± ì´í•´",
                    "ì²´ê³„ì  ë¦¬ìŠ¤í¬ ê´€ë¦¬"
                ]
                selected_factors = random.sample(success_factors, 3)
                response_parts.append("**í•µì‹¬ ì„±ê³µ ìš”ì¸:**")
                response_parts.extend([f"- {factor}" for factor in selected_factors])
            
            elif i == 2:  # ê¸°ëŒ€ íš¨ê³¼
                effects = [
                    "ê¸°ìˆ  ê²½ìŸë ¥ í–¥ìƒ ë° ì‹œì¥ í™•ëŒ€",
                    "ìƒí˜¸ ìœˆ-ìœˆ ê¸°ë°˜ ì§€ì† ì„±ì¥",
                    "ê¸€ë¡œë²Œ ê³µê¸‰ë§ ë‹¤ë³€í™”",
                    "ì‹ ê¸°ìˆ  ìœµí•©ì„ í†µí•œ í˜ì‹  ì°½ì¶œ"
                ]
                selected_effects = random.sample(effects, 2)
                response_parts.append("**ê¸°ëŒ€ íš¨ê³¼:**")
                response_parts.extend([f"- {effect}" for effect in selected_effects])
            
            else:  # ê²°ë¡ 
                conclusions = [
                    "ì„±ê³µì ì¸ ë°©ì‚° í˜‘ë ¥ì„ ìœ„í•´ì„œëŠ” ê¸°ìˆ ì  ìš°ìˆ˜ì„±ê³¼ ì „ëµì  íŒŒíŠ¸ë„ˆì‹­ì´ ê²°í•©ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.",
                    "ì¥ê¸°ì  ê´€ì ì—ì„œ ìƒí˜¸ ì´ìµì„ ì¶”êµ¬í•˜ëŠ” í˜‘ë ¥ ëª¨ë¸ êµ¬ì¶•ì´ í•µì‹¬ì…ë‹ˆë‹¤.",
                    "ê°êµ­ì˜ íŠ¹ì„±ì„ ê³ ë ¤í•œ ë§ì¶¤í˜• í˜‘ë ¥ ì „ëµ ìˆ˜ë¦½ì´ ì„±ê³µì˜ ì—´ì‡ ì…ë‹ˆë‹¤."
                ]
                response_parts.append(random.choice(conclusions))
            
            response_parts.append("")
        
        return "\n".join(response_parts)
    
    def _generate_general_response(self, topic_info: Dict, style: Dict, emojis: List[str]) -> str:
        """ì¼ë°˜ì ì¸ ì‘ë‹µ ìƒì„±"""
        keywords = topic_info["keywords"]
        main_topic = " ".join(keywords[:2]) if keywords else "í•´ë‹¹ ì£¼ì œ"
        
        opening = random.choice(style["opening"])
        sections = style["structure"]
        
        response_parts = [f'"{topic_info["query"]}"ì— ëŒ€í•´ {opening}\n']
        
        for i, section in enumerate(sections):
            emoji = emojis[i % len(emojis)]
            response_parts.append(f"## {emoji} {section}")
            
            if i == 0:
                response_parts.append(f"{main_topic}ì™€ ê´€ë ¨ëœ í˜„ì¬ ë™í–¥ì„ ì‚´í´ë³´ë©´:")
                response_parts.append(f"- ê¸€ë¡œë²Œ í™˜ê²½ ë³€í™”ì— ë”°ë¥¸ ìƒˆë¡œìš´ ê¸°íšŒ")
                response_parts.append(f"- ê¸°ìˆ  ë°œì „ê³¼ ì‹œì¥ ìˆ˜ìš”ì˜ ë³€í™”")
            
            elif i == 1:
                response_parts.append(f"í•µì‹¬ ê³ ë ¤ì‚¬í•­ë“¤:")
                response_parts.append(f"- {style['tone']} ì ‘ê·¼ì˜ ì¤‘ìš”ì„±")
                response_parts.append(f"- ë‹¤ì–‘í•œ ì´í•´ê´€ê³„ìë“¤ì˜ ìš”êµ¬ì‚¬í•­")
            
            elif i == 2:
                response_parts.append(f"í–¥í›„ ë°œì „ ë°©í–¥:")
                response_parts.append(f"- ì§€ì†ê°€ëŠ¥í•˜ê³  í˜ì‹ ì ì¸ ì ‘ê·¼")
                response_parts.append(f"- ë³€í™”í•˜ëŠ” í™˜ê²½ì— ëŒ€í•œ ëŠ¥ë™ì  ëŒ€ì‘")
            
            else:
                conclusions = [
                    f"{main_topic} ë¶„ì•¼ì—ì„œì˜ ì„±ê³µì„ ìœ„í•´ì„œëŠ” ì²´ê³„ì ì´ê³  ì „ëµì ì¸ ì ‘ê·¼ì´ í•„ìš”í•©ë‹ˆë‹¤.",
                    "ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ê³¼ ì ì‘ì„ í†µí•´ ìµœì ì˜ ê²°ê³¼ë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.",
                    "ë‹¤ì–‘í•œ ê´€ì ì„ ì¢…í•©í•œ ê· í˜•ì¡íŒ ì „ëµ ìˆ˜ë¦½ì´ ì¤‘ìš”í•©ë‹ˆë‹¤."
                ]
                response_parts.append(random.choice(conclusions))
            
            response_parts.append("")
        
        # ì „ë¬¸ ë¶„ì•¼ ì•ˆë‚´ ì¶”ê°€ (ì¼ë°˜ ì§ˆë¬¸ì¸ ê²½ìš°)
        if topic_info["category"] == "general":
            response_parts.append("---")
            response_parts.append("ğŸ’¡ **ì „ë¬¸ ë¶„ì•¼ ì•ˆë‚´**: ë°©ì‚° í˜‘ë ¥ ì „ëµ, ê¸°ìˆ  ì´ì „, êµ­ê°€ë³„ ì‹œì¥ ë¶„ì„ ë“±ì— ê´€í•œ ì§ˆë¬¸ì„ ì£¼ì‹œë©´ ë”ìš± ì „ë¬¸ì ì¸ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        return "\n".join(response_parts)


class DefenseCooperationChatbot:
    """ê°œì„ ëœ ë°©ì‚° í˜‘ë ¥ AI ì±—ë´‡ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.config = None
        self.kb = None
        self.prompt_engineer = None
        self.llama_system = None
        self.is_initialized = False
        # í•µì‹¬: ì§„ì§œ AIì²˜ëŸ¼ ë™ì‘í•˜ëŠ” ìƒì„±ê¸° ì‚¬ìš©
        self.intelligent_generator = TrueIntelligentGenerator()
        self.fallback_mode = True  # í•­ìƒ ë™ì  ì‘ë‹µ ìƒì„±
        
        # í†µê³„ ì¶”ì 
        self.conversation_count = 0
        self.response_history = []

    def initialize(self, use_gpu=False, use_quantization=False):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            logger.info("ğŸš€ ë°©ì‚° í˜‘ë ¥ AI ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")
            
            # ëª¨ë¸ ì„¤ì •
            self.config = ModelConfig()
            
            # ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶•
            logger.info("ğŸ“š ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶• ì¤‘...")
            try:
                self.kb = build_knowledge_base()
                logger.info("âœ… ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ")
            except:
                logger.warning("âš ï¸ ì™¸ë¶€ ì§€ì‹ ë² ì´ìŠ¤ ë¡œë”© ì‹¤íŒ¨, ë‚´ì¥ ì‹œìŠ¤í…œ ì‚¬ìš©")
                self.kb = None

            # í”„ë¡¬í”„íŠ¸ ì‹œìŠ¤í…œ êµ¬ì¶•
            logger.info("ğŸ”§ í”„ë¡¬í”„íŠ¸ ì‹œìŠ¤í…œ êµ¬ì¶• ì¤‘...")
            try:
                self.prompt_engineer = create_comprehensive_prompt_system(self.kb)
                logger.info("âœ… í”„ë¡¬í”„íŠ¸ ì‹œìŠ¤í…œ êµ¬ì¶• ì™„ë£Œ")
            except:
                logger.warning("âš ï¸ ì™¸ë¶€ í”„ë¡¬í”„íŠ¸ ì‹œìŠ¤í…œ ë¡œë”© ì‹¤íŒ¨, ë‚´ì¥ ì‹œìŠ¤í…œ ì‚¬ìš©")
                self.prompt_engineer = None

            # Llama ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œë„ (ì˜µì…˜)
            logger.info("ğŸ¤– ì™¸ë¶€ AI ëª¨ë¸ ë¡œë”© ì‹œë„ ì¤‘...")
            try:
                self.llama_system = DefenseCooperationLlama(
                    self.config, self.kb, self.prompt_engineer
                )
                self.llama_system.initialize_model()
                logger.info("âœ… ì™¸ë¶€ AI ëª¨ë¸ ë¡œë”© ì„±ê³µ")
            except:
                logger.info("ğŸ’¡ ì™¸ë¶€ AI ëª¨ë¸ ì—†ìŒ, ë‚´ì¥ ì§€ëŠ¥í˜• ì‹œìŠ¤í…œ ì‚¬ìš©")
                self.llama_system = None
            
            self.is_initialized = True
            logger.info("ğŸ‰ ì „ì²´ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì„±ê³µ!")
            logger.info("âœ… ì§„ì§œ AI ê°™ì€ ë™ì  ì‘ë‹µ ìƒì„± ì‹œìŠ¤í…œ í™œì„±í™”")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # ìµœì†Œí•œì˜ ê¸°ëŠ¥ìœ¼ë¡œë¼ë„ ë™ì‘
            self.is_initialized = True
            logger.info("âœ… ìµœì†Œ ê¸°ëŠ¥ ëª¨ë“œë¡œ ì‹œìŠ¤í…œ ë³µêµ¬ ì™„ë£Œ")
            return True

    def chat(self, user_input: str) -> str:
        """ê°„ë‹¨í•œ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ - í•­ìƒ ë™ì  ì‘ë‹µ"""
        if not self.is_initialized:
            return "âŒ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        self.conversation_count += 1
        
        try:
            # 1. ì™¸ë¶€ ì‹œìŠ¤í…œì´ ìˆìœ¼ë©´ ë¨¼ì € ì‹œë„
            if self.llama_system:
                try:
                    result = self.llama_system.generate_response(user_input)
                    if isinstance(result, dict):
                        response = result.get("response", "")
                        # ì‘ë‹µì´ ì œëŒ€ë¡œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
                        if response and len(response.strip()) > 50 and "Template" not in response:
                            return response
                except Exception as e:
                    logger.debug(f"ì™¸ë¶€ ì‹œìŠ¤í…œ ì‹¤íŒ¨: {e}")
            
            # 2. ë‚´ì¥ ì§€ëŠ¥í˜• ìƒì„±ê¸° ì‚¬ìš© (ë©”ì¸)
            response = self.intelligent_generator.generate_response(user_input)
            
            # 3. ì‘ë‹µ ê¸°ë¡
            self.response_history.append({
                "query": user_input,
                "response": response[:100] + "..." if len(response) > 100 else response,
                "timestamp": datetime.now().isoformat(),
                "length": len(response)
            })
            
            return response
                
        except Exception as e:
            logger.error(f"ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    def detailed_chat(self, user_input: str) -> dict:
        """ìƒì„¸ ì •ë³´ í¬í•¨ ì±„íŒ…"""
        if not self.is_initialized:
            return {"error": True, "response": "ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
        
        start_time = time.time()
        
        try:
            # 1. ì™¸ë¶€ ì‹œìŠ¤í…œ ì‹œë„
            external_response = None
            if self.llama_system:
                try:
                    result = self.llama_system.generate_response(user_input)
                    if isinstance(result, dict) and result.get("response"):
                        response = result["response"]
                        if len(response.strip()) > 50 and "Template" not in response:
                            external_response = result
                except Exception as e:
                    logger.debug(f"ì™¸ë¶€ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")
            
            # 2. ë‚´ì¥ ì‹œìŠ¤í…œ ì‚¬ìš©
            if not external_response:
                response = self.intelligent_generator.generate_response(user_input)
                generation_time = time.time() - start_time
                
                result = {
                    "query": user_input,
                    "response": response,
                    "generation_time": generation_time,
                    "model_info": {
                        "mode": "intelligent_dynamic_generation",
                        "source": "TrueIntelligentGenerator",
                        "version": "2.0"
                    },
                    "response_length": len(response),
                    "in_scope": True,
                    "conversation_count": self.conversation_count,
                    "unique_response": True
                }
            else:
                result = external_response
                result["backup_available"] = True
            
            # ì‘ë‹µ ê¸°ë¡
            self.response_history.append({
                "query": user_input,
                "timestamp": datetime.now().isoformat(),
                "mode": result.get("model_info", {}).get("mode", "unknown"),
                "length": result.get("response_length", 0)
            })
            
            return result
            
        except Exception as e:
            logger.error(f"ìƒì„¸ ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
            return {
                "error": True,
                "response": f"ìƒì„¸ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "query": user_input,
                "generation_time": time.time() - start_time,
                "model_info": {"mode": "error_handling"}
            }

    def get_diversity_stats(self) -> dict:
        """ë‹¤ì–‘ì„± í†µê³„ ì¡°íšŒ"""
        if len(self.response_history) < 2:
            return {
                "diversity_score": 1.0,
                "avg_similarity": 0.0,
                "total_responses": len(self.response_history),
                "rejected_count": 0,
                "conversation_count": self.conversation_count
            }
        
        # ê°„ë‹¨í•œ ë‹¤ì–‘ì„± ê³„ì‚°
        unique_modes = set(r.get("mode", "unknown") for r in self.response_history)
        diversity_score = min(1.0, len(unique_modes) / max(1, len(self.response_history) / 2))
        
        return {
            "diversity_score": diversity_score,
            "avg_similarity": 1.0 - diversity_score,
            "total_responses": len(self.response_history),
            "rejected_count": 0,
            "conversation_count": self.conversation_count,
            "unique_modes": len(unique_modes)
        }

    def reset_conversation(self):
        """ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
        self.conversation_count = 0
        self.response_history = []
        logger.info("ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def toggle_fallback_mode(self, enabled: bool = None) -> bool:
        """ë™ì  ì‘ë‹µ ìƒì„± ëª¨ë“œ í† ê¸€ (í•­ìƒ True ìœ ì§€)"""
        self.fallback_mode = True  # í•­ìƒ ë™ì  ì‘ë‹µ
        return self.fallback_mode

    def get_system_status(self) -> dict:
        """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""
        return {
            "dynamic_response_mode": True,
            "external_model_loaded": self.llama_system is not None,
            "knowledge_base_loaded": self.kb is not None,
            "prompt_system_loaded": self.prompt_engineer is not None,
            "response_generator": "TrueIntelligentGenerator v2.0",
            "system_initialized": self.is_initialized,
            "conversation_count": self.conversation_count,
            "response_history_size": len(self.response_history),
            "capabilities": [
                "ë™ì  ì‘ë‹µ ìƒì„±",
                "ë§¤ë²ˆ ë‹¤ë¥¸ êµ¬ì¡°ì™€ ë‚´ìš©",
                "ì§€ëŠ¥ì  ì§ˆë¬¸ ë¶„ë¥˜",
                "ë‹¤ì–‘í•œ ì‘ë‹µ ìŠ¤íƒ€ì¼"
            ]
        }


def interactive_mode():
    """ëŒ€í™”í˜• ëª¨ë“œ"""
    print("ğŸ¤– ë°©ì‚° í˜‘ë ¥ ì „ëµ AI ì–´ì‹œìŠ¤í„´íŠ¸ (ì™„ì „ ê°œì„  ë²„ì „)")
    print("=" * 70)
    
    print("ğŸ†• ê°œì„ ì‚¬í•­:")
    print("   â€¢ í•˜ë“œì½”ë”©ëœ ë‹µë³€ ì™„ì „ ì œê±°")
    print("   â€¢ ì§„ì§œ AIì²˜ëŸ¼ ë§¤ë²ˆ ë‹¤ë¥¸ ì‘ë‹µ ìƒì„±")
    print("   â€¢ ë™ì  ì§€ì‹ ê¸°ë°˜ ì‹œìŠ¤í…œ")
    print("   â€¢ ì§€ëŠ¥ì  ì§ˆë¬¸ ë¶„ë¥˜ ë° ë¶„ì„")
    print("=" * 70)
    
    chatbot = DefenseCooperationChatbot()
    
    try:
        chatbot.initialize(use_gpu=False, use_quantization=False)
        
        print("\nâœ… ì´ˆê¸°í™” ì™„ë£Œ! ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
        print("ëª…ë ¹ì–´:")
        print("  'ì¢…ë£Œ', 'quit', 'exit' - ì¢…ë£Œ")
        print("  'ìƒì„¸' - ë‹¤ìŒ ë‹µë³€ì— ìƒì„¸ ì •ë³´ í¬í•¨")
        print("  'ë„ì›€ë§' - ì¶”ì²œ ì§ˆë¬¸ ë³´ê¸°") 
        print("  'í†µê³„' - ì‹œìŠ¤í…œ í†µê³„ í™•ì¸")
        print("  'ìƒíƒœ' - ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸")
        print("  'í…ŒìŠ¤íŠ¸' - ë™ì¼ ì§ˆë¬¸ ë‹¤ì–‘ì„± í…ŒìŠ¤íŠ¸")
        print("=" * 70)

        detailed_mode = False
        question_count = 0
        
        while True:
            try:
                user_input = input("\nğŸ‘¤ ì§ˆë¬¸: ").strip()
                
                if user_input.lower() in ['ì¢…ë£Œ', 'quit', 'exit']:
                    print("ğŸ‘‹ ê°ì‚¬í•©ë‹ˆë‹¤!")
                    break
                    
                if user_input == 'ìƒì„¸':
                    detailed_mode = not detailed_mode
                    status = "ì¼œì§" if detailed_mode else "êº¼ì§"
                    print(f"ğŸ”§ ìƒì„¸ ëª¨ë“œ {status}")
                    continue
                
                if user_input == 'í…ŒìŠ¤íŠ¸':
                    print("\nğŸ§ª ë™ì¼ ì§ˆë¬¸ ë‹¤ì–‘ì„± í…ŒìŠ¤íŠ¸:")
                    test_question = "ë™ë‚¨ì•„ì‹œì•„ ë°©ì‚° í˜‘ë ¥ ì „ëµì€?"
                    print(f"ì§ˆë¬¸: {test_question}")
                    
                    for i in range(3):
                        print(f"\n--- ì‘ë‹µ {i+1} ---")
                        response = chatbot.chat(test_question)
                        sample = response[:200] + "..." if len(response) > 200 else response
                        print(sample)
                    continue
                
                if user_input == 'ìƒíƒœ':
                    status = chatbot.get_system_status()
                    print("\nğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ:")
                    for key, value in status.items():
                        if isinstance(value, list):
                            print(f"  - {key}: {', '.join(value)}")
                        else:
                            print(f"  - {key}: {value}")
                    continue
                
                if user_input == 'ë„ì›€ë§':
                    print("\nğŸ’¡ ì¶”ì²œ ì§ˆë¬¸ ì˜ˆì‹œ:")
                    print("  â€¢ ë™ë‚¨ì•„ì‹œì•„ ë°©ì‚° í˜‘ë ¥ ì „ëµì€?")
                    print("  â€¢ AIê°€ ë¯¸ë˜ ê¸°ìˆ ì— ì˜í–¥ì„ ë¼ì¹˜ì§€ ì•Šê²Œ í•˜ëŠ” ë°©ë²•ì€?")
                    print("  â€¢ UAEì™€ì˜ ë°©ì‚° íˆ¬ì í˜‘ë ¥ ë°©ì•ˆì€?")
                    print("  â€¢ ì¸ê³µì§€ëŠ¥ì˜ ìœ¤ë¦¬ì  ë¬¸ì œì ê³¼ í•´ê²°ì±…ì€?")
                    print("  â€¢ ì¤‘ë™ ì§€ì—­ ë°©ì‚° ìˆ˜ì¶œ ìš°ì„ ìˆœìœ„ëŠ”?")
                    continue
                
                if user_input == 'í†µê³„':
                    try:
                        stats = chatbot.get_diversity_stats()
                        print("\nğŸ“Š ì‹œìŠ¤í…œ í†µê³„:")
                        print(f"  - ëŒ€í™” íšŸìˆ˜: {stats.get('conversation_count', 0)}")
                        print(f"  - ì‘ë‹µ ë‹¤ì–‘ì„±: {stats.get('diversity_score', 0):.2f}")
                        print(f"  - ê³ ìœ  ì‘ë‹µ ëª¨ë“œ: {stats.get('unique_modes', 0)}ê°œ")
                        print(f"  - ì´ ì‘ë‹µ ìˆ˜: {stats.get('total_responses', 0)}")
                    except Exception as e:
                        print(f"âŒ í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {e}")
                    continue
                
                if not user_input:
                    continue

                question_count += 1
                print(f"\nğŸ¤– AI: ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ë§ì¶¤ ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤... (#{question_count})")
                
                try:
                    if detailed_mode:
                        result = chatbot.detailed_chat(user_input)
                        response = result.get("response", "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        
                        print("â”€" * 70)
                        print(response)
                        print("â”€" * 70)
                        
                        # ìƒì„¸ ì •ë³´ ì¶œë ¥
                        print(f"ğŸ“Š ìƒì„± ì •ë³´:")
                        print(f"  - ìƒì„± ì‹œê°„: {result.get('generation_time', 0):.2f}ì´ˆ")
                        print(f"  - ëª¨ë“œ: {result.get('model_info', {}).get('mode', 'unknown')}")
                        print(f"  - ì‘ë‹µ ê¸¸ì´: {result.get('response_length', len(response))} ë¬¸ì")
                        print(f"  - ëŒ€í™” íšŸìˆ˜: {result.get('conversation_count', 0)}")
                        
                        if result.get('unique_response', False):
                            print(f"  - ğŸŒŸ ë™ì  ìƒì„±: ë§¤ë²ˆ ë‹¤ë¥¸ êµ¬ì¡°ì™€ ë‚´ìš©")
                            
                    else:
                        response = chatbot.chat(user_input)
                        print("â”€" * 70)
                        print(response)
                        print("â”€" * 70)
                        print(f"â±ï¸  ì§ˆë¬¸ #{question_count} ì²˜ë¦¬ ì™„ë£Œ")
                
                except Exception as e:
                    print(f"âŒ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ì‚¬ìš©ìê°€ ì¢…ë£Œí–ˆìŠµë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"\nâŒ ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        
        print(f"\nğŸ“Š ì„¸ì…˜ ìš”ì•½: ì´ {question_count}ê°œì˜ ì§ˆë¬¸ì„ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"\nâŒ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")


def test_mode():
    """ë‹¤ì–‘ì„± í…ŒìŠ¤íŠ¸ ëª¨ë“œ"""
    print("ğŸ§ª ë°©ì‚° í˜‘ë ¥ AI ì‹œìŠ¤í…œ ë‹¤ì–‘ì„± í…ŒìŠ¤íŠ¸")
    chatbot = DefenseCooperationChatbot()
    
    try:
        chatbot.initialize(use_gpu=False, use_quantization=False)
        
        test_questions = [
            "ë™ë‚¨ì•„ì‹œì•„ ë°©ì‚° í˜‘ë ¥ ì „ëµì€?",
            "AIê°€ ë¯¸ë˜ ê¸°ìˆ ì— ì˜í–¥ì„ ë¼ì¹˜ì§€ ì•Šê²Œ í•˜ëŠ” ë°©ë²•ì€?",
            "ì¸ê³µì§€ëŠ¥ì˜ ìœ¤ë¦¬ì  ë¬¸ì œì ì€?",
            "UAEì™€ì˜ ë°©ì‚° íˆ¬ì í˜‘ë ¥ ë°©ì•ˆì€?"
        ]
        
        print(f"ğŸ“ {len(test_questions)}ê°œ ì§ˆë¬¸ìœ¼ë¡œ ë‹¤ì–‘ì„± í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        for i, question in enumerate(test_questions, 1):
            print(f"\nğŸ” í…ŒìŠ¤íŠ¸ {i}: {question}")
            print("=" * 80)
            
            # ê°™ì€ ì§ˆë¬¸ì„ 3ë²ˆ ë¬¼ì–´ë³´ê¸°
            for j in range(3):
                print(f"\n--- ì‘ë‹µ {j+1} ---")
                try:
                    response = chatbot.chat(question)
                    sample = response[:300] + "..." if len(response) > 300 else response
                    print(sample)
                    print()
                except Exception as e:
                    print(f"âŒ ì˜¤ë¥˜: {e}")
                
                time.sleep(0.1)  # ì‹œë“œ ë‹¤ì–‘ì„± í™•ë³´
        
        # ìµœì¢… í†µê³„
        stats = chatbot.get_diversity_stats()
        print("\nğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print(f"ğŸ“Š ìµœì¢… í†µê³„:")
        print(f"  - ì´ ì‘ë‹µ ìˆ˜: {stats.get('total_responses', 0)}")
        print(f"  - ë‹¤ì–‘ì„± ì ìˆ˜: {stats.get('diversity_score', 0):.2f}")
        print(f"  - ê³ ìœ  ëª¨ë“œ ìˆ˜: {stats.get('unique_modes', 0)}")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")


if __name__ == "__main__":
    print("ğŸŒŸ ë°©ì‚° í˜‘ë ¥ ì „ëµ AI ì‹œìŠ¤í…œ - ì™„ì „ ê°œì„  ë²„ì „")
    print("=" * 60)
    print("âœ… 1. í•˜ë“œì½”ë”©ëœ ì‘ë‹µ ì™„ì „ ì œê±°")
    print("âœ… 2. ì§„ì§œ AIì²˜ëŸ¼ ë§¤ë²ˆ ë‹¤ë¥¸ ì‘ë‹µ ìƒì„±") 
    print("âœ… 3. ë™ì  ì§€ì‹ ê¸°ë°˜ ì‹œìŠ¤í…œ")
    print("âœ… 4. ì§€ëŠ¥ì  ì§ˆë¬¸ ë¶„ë¥˜ ë° ë¶„ì„")
    print("=" * 60)
    
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        test_mode()
    else:
        interactive_mode()