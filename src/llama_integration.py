import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM
import json
import logging
from typing import Dict, List, Optional, Tuple
import time
from dataclasses import dataclass
import random
import re
from difflib import SequenceMatcher


logger = logging.getLogger(__name__)

@dataclass
class ModelConfig:
    """ìˆ˜ì •ëœ ëª¨ë¸ ì„¤ì • - T5 ëª¨ë¸ ì§€ì›"""
    model_name: str = "google/flan-t5-base"
    max_tokens: int = 512  # T5ì— ì í•©í•œ ê¸¸ì´ë¡œ ì¡°ì •
    temperature: float = 0.9
    top_p: float = 0.9
    do_sample: bool = True
    use_quantization: bool = False
    device_map: str = "auto"

class ResponseDiversityManager:
    """ì‘ë‹µ ë‹¤ì–‘ì„± ê²€ì¦ ë¡œì§"""
    
    def __init__(self, max_history=10, similarity_threshold=0.65):
        self.response_history = []
        self.max_history = max_history
        self.similarity_threshold = similarity_threshold
        self.rejected_responses = []
    
    def add_response(self, query: str, response: str):
        """ì‘ë‹µ ê¸°ë¡ ì¶”ê°€"""
        self.response_history.append({
            "query": query,
            "response": response,
            "timestamp": time.time(),
            "keywords": self._extract_keywords(response)
        })
        
        if len(self.response_history) > self.max_history:
            self.response_history.pop(0)
    
    def _extract_keywords(self, text: str) -> List[str]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = []
        defense_terms = ["ë¯¸ì‚¬ì¼", "ë°©ê³µ", "í•­ê³µ", "í•´êµ°", "í˜‘ë ¥", "íˆ¬ì", "ìˆ˜ì¶œ", 
                        "ê¸°ìˆ ì´ì „", "ë¬´ì¸", "ë“œë¡ ", "ë ˆì´ë”", "ì‚¬ì´ë²„", "AI", "ê°œë°œ",
                        "ì¸ë„", "UAE", "ë¸Œë¼ì§ˆ", "ë™ë‚¨ì•„", "ì¤‘ë™", "ì•„í”„ë¦¬ì¹´"]
        
        for term in defense_terms:
            if term in text:
                keywords.append(term)
        return keywords
    
    def check_similarity(self, new_response: str) -> Tuple[bool, float]:
        """ì‘ë‹µ ìœ ì‚¬ë„ ê²€ì‚¬"""
        if not self.response_history:
            return False, 0.0
        
        max_similarity = 0.0
        for record in self.response_history[-5:]:
            similarity = SequenceMatcher(None, new_response, record["response"]).ratio()
            max_similarity = max(max_similarity, similarity)
            
            if similarity > self.similarity_threshold:
                return True, similarity
        
        return False, max_similarity
    
    def get_diversity_metrics(self) -> Dict:
        """ë‹¤ì–‘ì„± ë©”íŠ¸ë¦­ ê³„ì‚°"""
        if len(self.response_history) < 2:
            return {"diversity_score": 1.0, "avg_similarity": 0.0}
        
        similarities = []
        responses = [r["response"] for r in self.response_history[-5:]]
        
        for i in range(len(responses)):
            for j in range(i+1, len(responses)):
                sim = SequenceMatcher(None, responses[i], responses[j]).ratio()
                similarities.append(sim)
        
        avg_similarity = sum(similarities) / len(similarities) if similarities else 0
        diversity_score = 1.0 - avg_similarity
        
        return {
            "diversity_score": diversity_score,
            "avg_similarity": avg_similarity,
            "total_responses": len(self.response_history),
            "rejected_count": len(self.rejected_responses)
        }

class EnhancedKnowledgeBase:
    """í–¥ìƒëœ ì§€ì‹ ë² ì´ìŠ¤ - PDF ë°ì´í„° ë°˜ì˜"""
    
    def __init__(self, knowledge_base):
        self.kb = knowledge_base
        self.qa_pairs = self._load_pdf_qa_data()
        
    def _load_pdf_qa_data(self) -> List[Dict]:
        """PDFì˜ ì§ˆë¬¸-ë‹µë³€ ë°ì´í„° ë¡œë“œ"""
        return [
            {
                "question": "ì¤‘ë™ ë° ë¶ì•„í”„ë¦¬ì¹´ ì§€ì—­ì—ì„œ í•œêµ­ì˜ ë°©ì‚° ìˆ˜ì¶œ ìš°ì„ ìˆœìœ„ êµ­ê°€ë¥¼ ìˆœìœ„ë³„ë¡œ ì•Œë ¤ì£¼ì„¸ìš”",
                "answer": """ì¤‘ë™ ë° ë¶ì•„í”„ë¦¬ì¹´ ì§€ì—­ì˜ ë°©ì‚° ìˆ˜ì¶œ ìš°ì„ ìˆœìœ„ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

**1ìˆœìœ„: UAE (ì•„ëì—ë¯¸ë¦¬íŠ¸)**
- ë°©ìœ„ì‚°ì—… íˆ¬ì: ì—°ê°„ ì•½ 220ì–µ ë‹¬ëŸ¬
- ì¤‘ì  ê¸°ìˆ  ì˜ì—­: ë¯¸ì‚¬ì¼ ë°©ì–´, ì „ìì „, ë¬´ì¸ ì‹œìŠ¤í…œ
- ìƒë³´ì  ê¸°ìˆ : ê³ ê¸‰ ë ˆì´ë” ì‹œìŠ¤í…œ, ì „ìì •ë³´ ì‹œìŠ¤í…œ
- ì‹œì¥ ê¸°íšŒ: ê¸°ìˆ  ë‹¤ë³€í™” ì¶”ì§„ìœ¼ë¡œ ìƒˆë¡œìš´ íŒŒíŠ¸ë„ˆ ëª¨ìƒ‰ ì¤‘
- í˜‘ë ¥ ìš©ì´ì„±: ë†’ìŒ (í•œ-UAE íŠ¹ë³„ ì „ëµì  íŒŒíŠ¸ë„ˆì‹­)

**2ìˆœìœ„: ì´ì§‘íŠ¸**
- ë°©ìœ„ì‚°ì—… íˆ¬ì: ì—°ê°„ ì•½ 50ì–µ ë‹¬ëŸ¬ (ì•„í”„ë¦¬ì¹´ ìµœëŒ€)
- ì¤‘ì  ê¸°ìˆ  ì˜ì—­: ì‚¬ë§‰ í™˜ê²½ ìš´ìš©, ë°©ê³µë§ ìš´ìš©, ëŒ€í…ŒëŸ¬ ì¥ë¹„
- ìƒë³´ì  ê¸°ìˆ : ê·¹í•œ ì‚¬ë§‰í™˜ê²½ ì¥ë¹„ ìš´ìš© ë…¸í•˜ìš°
- ì‹œì¥ ê¸°íšŒ: K9 ìì£¼í¬ ìˆ˜ì¶œ ë“± ê¸°ì¡´ í˜‘ë ¥ ê²½í—˜

**3ìˆœìœ„: ì¹´íƒ€ë¥´**
- ë°©ìœ„ì‚°ì—… íˆ¬ì: ì¤‘ê°„ ê·œëª¨ì´ë‚˜ êµ¬ë§¤ë ¥ ìš°ìˆ˜
- ì¤‘ì  ê¸°ìˆ  ì˜ì—­: ë°©ê³µ, í•´ì–‘ ë³´ì•ˆ
- í˜‘ë ¥ ì¥ë²½: ì§€ì—­ ì •ì¹˜ì  ë³µì¡ì„±

**4ìˆœìœ„: ëª¨ë¡œì½”**
- ë°©ìœ„ì‚°ì—… íˆ¬ì: ìƒëŒ€ì ìœ¼ë¡œ ì œí•œì 
- ì¤‘ì  ê¸°ìˆ  ì˜ì—­: êµ­ê²½ ê°ì‹œ, ëŒ€í…ŒëŸ¬
- ì‹œì¥ ê¸°íšŒ: ì•„í”„ë¦¬ì¹´ ì§„ì¶œ êµë‘ë³´ ê°€ëŠ¥""",
                "category": "ì§€ì—­ë³„ ìš°ì„ ìˆœìœ„"
            },
            {
                "question": "ì¸ë„ì™€ì˜ ë¯¸ì‚¬ì¼ ê¸°ìˆ  í˜‘ë ¥ ì „ëµì€",
                "answer": """### ğŸš€ í˜„ë¬´-BrahMos í•©ë™ ë¯¸ì‚¬ì¼ ê°œë°œ í”„ë¡œê·¸ë¨
- **ì¸ë„ êµ­ë°©ì˜ˆì‚°**: 730ì–µ ë‹¬ëŸ¬ (2024ë…„ ê¸°ì¤€, ì„¸ê³„ 3ìœ„)
- **BrahMos ê¸°ìˆ ë ¥**: ë§ˆí•˜ 2.8~3.0 ì´ˆìŒì† ìˆœí•­ë¯¸ì‚¬ì¼, ëŸ¬ì‹œì•„ í•©ì‘ ì„±ê³µì‚¬ë¡€
- **í˜„ë¬´ ì‹œë¦¬ì¦ˆ**: ì‚¬ê±°ë¦¬ 300-800km, í•œêµ­ ë…ìê°œë°œ ì •ë°€íƒ€ê²© ë¬´ê¸°ì²´ê³„
- **ì‹œì¥ ì ì¬ë ¥**: ë™ë‚¨ì•„-ì¤‘ë™ ì •ë°€íƒ€ê²© ì‹œì¥ ì—° 5.8% ì„±ì¥, 118ì–µ ë‹¬ëŸ¬ ê·œëª¨

### ğŸ’° 3ë‹¨ê³„ íˆ¬ì ê³„íš (ì´ 23ì–µ ë‹¬ëŸ¬)
**1ë‹¨ê³„: ê³µë™ì—°êµ¬ê°œë°œ (2025-2026, 3ì–µ ë‹¬ëŸ¬)**
- í•œêµ­ íˆ¬ì: 1.8ì–µ ë‹¬ëŸ¬ (ì¶”ì§„ì²´, ì‹œìŠ¤í…œ í†µí•©)
- ì¸ë„ íˆ¬ì: 1.2ì–µ ë‹¬ëŸ¬ (ìœ ë„ì‹œìŠ¤í…œ, AI ì†Œí”„íŠ¸ì›¨ì–´)
- ë¶€ì‚°-ì²¸ë‚˜ì´ Twin R&D Center ì„¤ë¦½

**2ë‹¨ê³„: í”„ë¡œí† íƒ€ì… ê°œë°œ (2026-2028, 8ì–µ ë‹¬ëŸ¬)**
- 50:50 íˆ¬ì ë¶„ë‹´ (ê° 4ì–µ ë‹¬ëŸ¬)
- ëª©í‘œ ì„±ëŠ¥: ì‚¬ê±°ë¦¬ 1,500km, CEP 1m ì´í•˜

**3ë‹¨ê³„: ì–‘ì‚° ë° ìˆ˜ì¶œ (2028-2030, 12ì–µ ë‹¬ëŸ¬)**
- í•œêµ­ ìƒì‚°ë¶„: 5ì–µ ë‹¬ëŸ¬ (ì—°ê°„ 80ê¸°)
- ì¸ë„ ìƒì‚°ë¶„: 7ì–µ ë‹¬ëŸ¬ (ì—°ê°„ 120ê¸°, Make in India 60% í˜„ì§€í™”)

### ğŸ“Š íˆ¬ììˆ˜ìµë¥  ë¶„ì„ (10ë…„ ê¸°ì¤€)
- **ì´ ROI**: 332% (íšŒìˆ˜ê¸°ê°„ 4.8ë…„)
- **ê³ ìš©ì°½ì¶œ**: ì§ê°„ì ‘ 15,000ëª…""",
                "category": "ê¸°ìˆ  í˜‘ë ¥"
            },
            {
                "question": "UAE íˆ¬ì ê·œëª¨",
                "answer": """### ğŸ‡¦ğŸ‡ª UAEì™€ í•œêµ­ì˜ ê¸°ìˆ  í†µí•© ì „ëµ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸

## 1. ì‚¬ë§‰í™˜ê²½ ìµœì í™” í†µí•© ë°©ê³µì‹œìŠ¤í…œ
**í˜‘ë ¥ êµ¬ì¡°:**
- í•œêµ­: ì²œê¶ ë¯¸ì‚¬ì¼ ì‹œìŠ¤í…œ + ì‹œìŠ¤í…œ í†µí•© ê¸°ìˆ 
- UAE: ê³ ì˜¨í™˜ê²½ ì ì‘ ê¸°ìˆ  + í˜„ì§€ ë§ì¶¤í™” ê¸°ìˆ 
- íˆ¬ì ê·œëª¨: ì´ 7ì–µ ë‹¬ëŸ¬ (í•œêµ­ 4ì–µ, UAE 3ì–µ)
- ìƒì‚° ë¶„ë‹´: í•œêµ­ 60%, UAE 40%

**ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸:**
- Phase 1: ê³µë™ê°œë°œ (2ë…„, 2ì–µ ë‹¬ëŸ¬)
- Phase 2: ì‹œì œí’ˆ ì œì‘ ë° ì‹œí—˜ (1ë…„, 1.5ì–µ ë‹¬ëŸ¬)
- Phase 3: ì–‘ì‚° ë° GCC ì§€ì—­ ë§ˆì¼€íŒ… (3ë…„, 3.5ì–µ ë‹¬ëŸ¬)

**ì˜ˆìƒ íš¨ê³¼:**
- ê²½ì œì : 10ë…„ê°„ 90ì–µ ë‹¬ëŸ¬ ìˆ˜ì¶œ ì°½ì¶œ
- ê¸°ìˆ ì : ê·¹í•œí™˜ê²½ ê¸°ìˆ  í™•ë³´ë¡œ ê¸€ë¡œë²Œ ê²½ìŸë ¥ ê°•í™”
- ì „ëµì : ì¤‘ë™ ë°©ì‚° í˜‘ë ¥ í—ˆë¸Œ êµ¬ì¶•""",
                "category": "ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸"
            }
        ]
    
    def find_relevant_answer(self, query: str) -> Optional[str]:
        """ì§ˆë¬¸ì— ê´€ë ¨ëœ ë‹µë³€ ì°¾ê¸°"""
        query_lower = query.lower()
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ê´€ë ¨ ë‹µë³€ ì°¾ê¸°
        for qa in self.qa_pairs:
            question_keywords = qa["question"].lower()
            if any(keyword in query_lower for keyword in question_keywords.split()):
                return qa["answer"]
        
        return None
    
    def is_question_in_scope(self, query: str) -> bool:
        """ì§ˆë¬¸ì´ ì§€ì‹ ë² ì´ìŠ¤ ë²”ìœ„ ë‚´ì¸ì§€ í™•ì¸"""
        defense_keywords = [
            "ë°©ì‚°", "ë¯¸ì‚¬ì¼", "ë°©ì–´", "êµ°ì‚¬", "ë¬´ê¸°", "í˜‘ë ¥", "ìˆ˜ì¶œ", "íˆ¬ì",
            "ì¸ë„", "UAE", "ë¸Œë¼ì§ˆ", "ì¤‘ë™", "ë™ë‚¨ì•„", "ì•„í”„ë¦¬ì¹´", "ê¸°ìˆ ì´ì „",
            "ì‚¬ì´ë²„", "ìš°ì£¼", "í•­ê³µ", "í•´ì–‘", "AI", "ë“œë¡ "
        ]
        
        query_lower = query.lower()
        return any(keyword in query_lower for keyword in defense_keywords)
class IntelligentResponseGenerator:
    """ì§€ëŠ¥í˜• ì‘ë‹µ ìƒì„±ê¸° - íŒŒì¸íŠœë‹ ë°ì´í„° ì™¸ì—ë„ ìì²´ ë‹µë³€ ìƒì„±"""
    
    def __init__(self, knowledge_base, model_config):
        self.kb = knowledge_base
        self.config = model_config
        self.general_knowledge_templates = self._build_general_templates()
        self.reasoning_patterns = self._build_reasoning_patterns()
        
    def _build_general_templates(self) -> Dict[str, List[str]]:
        """ì¼ë°˜ì ì¸ ì§€ì‹ ê¸°ë°˜ í…œí”Œë¦¿ë“¤"""
        return {
            "technology_analysis": [
                """í•´ë‹¹ ê¸°ìˆ  ë¶„ì•¼ì— ëŒ€í•œ ë¶„ì„:

### ğŸ” ê¸°ìˆ  í˜„í™©
- í˜„ì¬ ê¸€ë¡œë²Œ ê¸°ìˆ  íŠ¸ë Œë“œë¥¼ ê³ ë ¤í•  ë•Œ, ì´ ë¶„ì•¼ëŠ” {trend_analysis}
- ì£¼ìš” ê¸°ìˆ  ì„ ë„êµ­ë“¤ì˜ ì ‘ê·¼ ë°©ì‹: {tech_approach}

### ğŸ¯ í•œêµ­ì˜ ê¸°ìˆ ì  ìœ„ì¹˜
- í•œêµ­ì˜ í˜„ì¬ ê¸°ìˆ  ìˆ˜ì¤€: {korea_level}
- ê°•ì  ë¶„ì•¼: {strengths}
- ê°œì„ ì´ í•„ìš”í•œ ì˜ì—­: {improvement_areas}

### ğŸ’¡ ë°œì „ ì „ëµ
- ë‹¨ê¸° ëª©í‘œ (1-2ë…„): {short_term}
- ì¤‘ê¸° ëª©í‘œ (3-5ë…„): {medium_term}
- ì¥ê¸° ë¹„ì „ (5-10ë…„): {long_term}

### ğŸ“Š ê¸°ëŒ€ íš¨ê³¼
- ê¸°ìˆ ì  íš¨ê³¼: {tech_benefits}
- ê²½ì œì  íš¨ê³¼: {economic_benefits}
- ì „ëµì  ì˜ë¯¸: {strategic_meaning}"""
            ],
            "general_inquiry": [
                """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€:

### ğŸ” í˜„í™© ë¶„ì„
{current_analysis}

### ğŸ’¡ ì£¼ìš” ê´€ì 
{key_perspectives}

### ğŸ“ˆ ë°œì „ ë°©í–¥
{development_direction}

### ğŸ¯ ê¶Œì¥ì‚¬í•­
{recommendations}"""
            ]
        }
    
    def _build_reasoning_patterns(self) -> Dict[str, callable]:
        """ì¶”ë¡  íŒ¨í„´ë“¤"""
        return {
            "technology_reasoning": self._reason_about_technology,
            "general_reasoning": self._reason_generally
        }
    
    def _reason_about_technology(self, topic: str, context: str) -> Dict[str, str]:
        """ê¸°ìˆ  ê´€ë ¨ ì¶”ë¡ """
        return {
            "trend_analysis": f"{topic} ê¸°ìˆ ì€ í˜„ì¬ AI, ìë™í™”, ë””ì§€í„¸ ì „í™˜ì˜ ì˜í–¥ì„ ë°›ì•„ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
            "tech_approach": "ì„ ì§„êµ­ë“¤ì€ ë¯¼ê´€ í˜‘ë ¥, ì˜¤í”ˆ ì´ë…¸ë² ì´ì…˜, êµ­ì œ ê³µë™ì—°êµ¬ë¥¼ í†µí•´ ê¸°ìˆ  ë°œì „ì„ ì¶”ì§„í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
            "korea_level": "í•œêµ­ì€ ì œì¡°ì—… ê¸°ë°˜ê³¼ IT ì¸í”„ë¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¤‘ìƒìœ„ê¶Œì˜ ê¸°ìˆ  ìˆ˜ì¤€ì„ ë³´ìœ í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
            "strengths": "ì‹œìŠ¤í…œ í†µí•©, ëŒ€ëŸ‰ ìƒì‚°, í’ˆì§ˆ ê´€ë¦¬ ë¶„ì•¼ì—ì„œ ê°•ì ì„ ë³´ì…ë‹ˆë‹¤.",
            "improvement_areas": "ê¸°ì´ˆ ì—°êµ¬, ì›ì²œ ê¸°ìˆ , ê¸€ë¡œë²Œ í‘œì¤€í™” ì£¼ë„ë ¥ í–¥ìƒì´ í•„ìš”í•©ë‹ˆë‹¤.",
            "short_term": "ê¸°ì¡´ ê¸°ìˆ ì˜ ê³ ë„í™” ë° íŒŒì¼ëŸ¿ í”„ë¡œì íŠ¸ ì¶”ì§„",
            "medium_term": "í•µì‹¬ ê¸°ìˆ  ìë¦½í™” ë° êµ­ì œ í˜‘ë ¥ í™•ëŒ€",
            "long_term": "ê¸€ë¡œë²Œ ê¸°ìˆ  ì„ ë„êµ­ ì§„ì… ë° í‘œì¤€ ì£¼ë„",
            "tech_benefits": "ê¸°ìˆ  ê²½ìŸë ¥ í–¥ìƒ, í˜ì‹  ìƒíƒœê³„ êµ¬ì¶•",
            "economic_benefits": "ìƒˆë¡œìš´ ì‚°ì—… ì°½ì¶œ, ìˆ˜ì¶œ ì¦ëŒ€, ì¼ìë¦¬ ì°½ì¶œ",
            "strategic_meaning": "ê¸°ìˆ  ì£¼ê¶Œ í™•ë³´, êµ­ê°€ ì•ˆë³´ ê°•í™”, ê¸€ë¡œë²Œ ì˜í–¥ë ¥ í™•ëŒ€"
        }
    
    def _reason_generally(self, topic: str, context: str) -> Dict[str, str]:
        """ì¼ë°˜ì  ì¶”ë¡ """
        return {
            "current_analysis": f"'{topic}'ì— ëŒ€í•œ í˜„ì¬ ìƒí™©ì„ ë¶„ì„í•˜ë©´, ë‹¤ì–‘í•œ ìš”ì¸ë“¤ì´ ë³µí•©ì ìœ¼ë¡œ ì‘ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
            "key_perspectives": "ê¸°ìˆ ì , ê²½ì œì , ì‚¬íšŒì  ê´€ì ì—ì„œ ì¢…í•©ì ìœ¼ë¡œ ì ‘ê·¼í•´ì•¼ í•©ë‹ˆë‹¤.",
            "development_direction": "ì§€ì†ê°€ëŠ¥í•˜ê³  í˜ì‹ ì ì¸ ë°©í–¥ìœ¼ë¡œ ë°œì „í•´ ë‚˜ê°€ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.",
            "recommendations": "ì²´ê³„ì ì¸ ê³„íš ìˆ˜ë¦½ê³¼ ë‹¨ê³„ì  ì‹¤í–‰ì„ í†µí•´ ëª©í‘œë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤."
        }
    
    def classify_question_type(self, query: str) -> str:
        """ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜"""
        technology_keywords = ["ê¸°ìˆ ", "ê°œë°œ", "í˜ì‹ ", "ì—°êµ¬", "AI", "ì¸ê³µì§€ëŠ¥", "ë¡œë´‡", "ìë™í™”"]
        
        query_lower = query.lower()
        
        if any(keyword in query_lower for keyword in technology_keywords):
            return "technology_reasoning"
        else:
            return "general_reasoning"
    
    def extract_key_topic(self, query: str) -> str:
        """ì§ˆë¬¸ì—ì„œ í•µì‹¬ ì£¼ì œ ì¶”ì¶œ"""
        important_words = []
        words = query.split()
        
        stop_words = {"ì˜", "ëŠ”", "ì´", "ê°€", "ì„", "ë¥¼", "ì—", "ì—ì„œ", "ë¡œ", "ìœ¼ë¡œ", "ì™€", "ê³¼", "í•˜ê³ ", "ì–´ë–»ê²Œ", "ì™œ", "ë¬´ì—‡"}
        
        for word in words:
            if len(word) > 1 and word not in stop_words:
                important_words.append(word)
        
        return " ".join(important_words[:3])
    
    def generate_intelligent_response(self, query: str, context: str = "") -> str:
        """ì§€ëŠ¥ì  ì‘ë‹µ ìƒì„±"""
        question_type = self.classify_question_type(query)
        key_topic = self.extract_key_topic(query)
        
        reasoning_func = self.reasoning_patterns[question_type]
        reasoning_results = reasoning_func(key_topic, context)
        
        if question_type == "technology_reasoning":
            template = self.general_knowledge_templates["technology_analysis"][0]
            try:
                response = template.format(**reasoning_results)
            except KeyError:
                response = self._generate_simple_response(query, key_topic)
        else:
            template = self.general_knowledge_templates["general_inquiry"][0]
            try:
                response = template.format(**reasoning_results)
            except KeyError:
                response = self._generate_simple_response(query, key_topic)
        
        return response
    
    def _generate_simple_response(self, query: str, topic: str) -> str:
        """ê°„ë‹¨í•œ ì‘ë‹µ ìƒì„±"""
        return f"""'{topic}'ì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

### ğŸ” ë¶„ì„
í•´ë‹¹ ì£¼ì œì— ëŒ€í•´ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì ‘ê·¼í•´ë³´ê² ìŠµë‹ˆë‹¤.

### ğŸ’¡ ì£¼ìš” ê³ ë ¤ì‚¬í•­
1. **í˜„ì¬ ìƒí™©**: ìµœì‹  ë™í–¥ê³¼ í˜„í™©ì„ íŒŒì•…í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤
2. **ë°œì „ ë°©í–¥**: ë¯¸ë˜ ì§€í–¥ì ì´ê³  ì§€ì†ê°€ëŠ¥í•œ ì ‘ê·¼ì´ í•„ìš”í•©ë‹ˆë‹¤
3. **ì‹¤í–‰ ë°©ì•ˆ**: êµ¬ì²´ì ì´ê³  ì‹¤í˜„ ê°€ëŠ¥í•œ ê³„íš ìˆ˜ë¦½ì´ í•µì‹¬ì…ë‹ˆë‹¤

### ğŸ“ˆ ê¶Œì¥ì‚¬í•­
ì²´ê³„ì ì¸ ë¶„ì„ê³¼ ë‹¨ê³„ì  ì ‘ê·¼ì„ í†µí•´ ëª©í‘œë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.

*ë” êµ¬ì²´ì ì¸ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ ì„¸ë¶€ ì‚¬í•­ì„ ëª…ì‹œí•´ ì£¼ì‹œë©´ ë³´ë‹¤ ìƒì„¸í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.*"""

class DefenseCooperationLlama:
    """í–¥ìƒëœ ë°©ì‚° í˜‘ë ¥ LLM ì‹œìŠ¤í…œ - ìì²´ ë‹µë³€ ìƒì„± ê¸°ëŠ¥ ì¶”ê°€"""
    
    def __init__(self, config: ModelConfig, knowledge_base, prompt_engineer):
        self.config = config
        self.kb = knowledge_base
        self.prompt_engineer = prompt_engineer
        self.tokenizer = None
        self.model = None 
        self.conversation_history = []
        
        # í–¥ìƒëœ êµ¬ì„± ìš”ì†Œë“¤
        self.diversity_manager = ResponseDiversityManager()
        self.enhanced_kb = EnhancedKnowledgeBase(knowledge_base)
        self.used_templates = []
        
        # ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶”ê°€
        self.intelligent_generator = IntelligentResponseGenerator(knowledge_base, config)
        self.fallback_mode = True  # ìì²´ ë‹µë³€ ìƒì„± ëª¨ë“œ í™œì„±í™”

    def initialize_model(self):
        """ëª¨ë¸ ì´ˆê¸°í™” - T5 ëª¨ë¸ ì§€ì›"""
        try:
            logger.info(f"T5 model loading: {self.config.model_name}")
            
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config.model_name,
                trust_remote_code=True
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            if "t5" in self.config.model_name.lower():
                self.model = AutoModelForSeq2SeqLM.from_pretrained(
                    self.config.model_name,
                    device_map="cpu",
                    torch_dtype=torch.float32,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
            else:
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.config.model_name,
                    device_map="cpu",
                    torch_dtype=torch.float32,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
            
            logger.info("âœ… Model initialized successfully")
            
        except Exception as e:
            logger.error(f"âŒ Model initialization failed: {e}")
            self._setup_dummy_mode()

    def _setup_dummy_mode(self):
        """ë”ë¯¸ ëª¨ë“œ ì„¤ì •"""
        self.model = "dummy_model"
        self.tokenizer = "dummy_tokenizer"
        logger.info("âœ… Dummy mode activated")

    def generate_response(self, user_query: str, use_history: bool = True) -> Dict:
        """í–¥ìƒëœ ì‘ë‹µ ìƒì„± - ìì²´ ë‹µë³€ ìƒì„± ê¸°ëŠ¥ í¬í•¨"""
        start_time = time.time()
        
        try:
            # 1. ê¸°ì¡´ ì§€ì‹ ë² ì´ìŠ¤ì—ì„œ ë‹µë³€ ì°¾ê¸°
            relevant_answer = self.enhanced_kb.find_relevant_answer(user_query)
            if relevant_answer:
                response = relevant_answer
                self.diversity_manager.add_response(user_query, response)
                
                return {
                    "query": user_query,
                    "response": response,
                    "generation_time": time.time() - start_time,
                    "model_info": {
                        "model_name": self.config.model_name,
                        "mode": "knowledge_base",
                        "source": "pdf_data"
                    },
                    "response_length": len(response),
                    "in_scope": True
                }
            
            # 2. ë°©ì‚° ê´€ë ¨ ì§ˆë¬¸ì¸ì§€ í™•ì¸
            is_defense_related = self.enhanced_kb.is_question_in_scope(user_query)
            
            # 3. ë°©ì‚° ê´€ë ¨ ì§ˆë¬¸ì´ë©´ ê¸°ì¡´ ë¡œì§ ì‚¬ìš©
            if is_defense_related:
                context_info = self._get_context_from_kb(user_query)
                
                if self.model == "dummy_model" or self.model is None:
                    response = self._generate_knowledge_based_response(user_query, context_info)
                    mode = "enhanced_dummy"
                else:
                    response = self._generate_real_response(user_query, context_info)
                    mode = "real_model"
                
                self.diversity_manager.add_response(user_query, response)
                
                return {
                    "query": user_query,
                    "response": response,
                    "generation_time": time.time() - start_time,
                    "model_info": {
                        "model_name": self.config.model_name,
                        "mode": mode,
                        "temperature": self.config.temperature
                    },
                    "response_length": len(response),
                    "in_scope": True
                }
            
            # 4. ë°©ì‚° ì™¸ ì§ˆë¬¸ì´ì§€ë§Œ ìì²´ ë‹µë³€ ìƒì„± ëª¨ë“œê°€ í™œì„±í™”ëœ ê²½ìš°
            elif self.fallback_mode:
                response = self.intelligent_generator.generate_intelligent_response(
                    user_query, 
                    context=""
                )
                
                # ë°©ì‚° ê´€ë ¨ì„± ì•ˆë‚´ ì¶”ê°€
                response = f"""**[ì¼ë°˜ ì£¼ì œ ë‹µë³€]**

{response}

---
ğŸ’¡ **ì°¸ê³ **: ì´ ì§ˆë¬¸ì€ ë°©ì‚° í˜‘ë ¥ ë¶„ì•¼ë¥¼ ë²—ì–´ë‚œ ë‚´ìš©ì…ë‹ˆë‹¤. ë°©ì‚° ìˆ˜ì¶œ, ê¸°ìˆ  í˜‘ë ¥, êµ­ê°€ë³„ ì „ëµ ë“±ì— ê´€í•œ ì§ˆë¬¸ì„ ì£¼ì‹œë©´ ë” ì „ë¬¸ì ì¸ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."""
                
                return {
                    "query": user_query,
                    "response": response,
                    "generation_time": time.time() - start_time,
                    "model_info": {"mode": "intelligent_fallback", "source": "general_knowledge"},
                    "in_scope": False,
                    "fallback_used": True
                }
            
            # 5. ìì²´ ë‹µë³€ ìƒì„± ëª¨ë“œê°€ ë¹„í™œì„±í™”ëœ ê²½ìš° (ê¸°ì¡´ ë°©ì‹)
            else:
                return {
                    "query": user_query,
                    "response": "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì§ˆë¬¸ì€ ë°©ì‚° í˜‘ë ¥ ì „ëµ ë¶„ì•¼ë¥¼ ë²—ì–´ë‚œ ë‚´ìš©ìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. ë°©ì‚° ìˆ˜ì¶œ, ê¸°ìˆ  í˜‘ë ¥, êµ­ê°€ë³„ ì „ëµ ë“±ì— ê´€ë ¨ëœ ì§ˆë¬¸ì„ í•´ì£¼ì‹œë©´ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                    "generation_time": time.time() - start_time,
                    "model_info": {"mode": "out_of_scope"},
                    "in_scope": False
                }
                
        except Exception as e:
            logger.error(f"Response generation error: {e}")
            
            # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ìì²´ ë‹µë³€ ìƒì„± ì‹œë„
            if self.fallback_mode:
                try:
                    fallback_response = self.intelligent_generator.generate_intelligent_response(
                        user_query, 
                        f"ì˜¤ë¥˜ ë°œìƒìœ¼ë¡œ ì¸í•œ ëŒ€ì²´ ì‘ë‹µ"
                    )
                    return {
                        "query": user_query,
                        "response": f"ì‹œìŠ¤í…œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì§€ë§Œ ìµœì„ ì˜ ë‹µë³€ì„ ì œê³µí•´ë“œë¦½ë‹ˆë‹¤:\n\n{fallback_response}",
                        "generation_time": time.time() - start_time,
                        "model_info": {"mode": "error_fallback"},
                        "error_handled": True
                    }
                except:
                    pass
            
            return {
                "query": user_query,
                "response": "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.",
                "error": True,
                "generation_time": time.time() - start_time
            }

    def _get_context_from_kb(self, query: str) -> str:
        """ì§€ì‹ ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        context_parts = []
        
        for country_name, profile in self.kb.countries.items():
            if country_name in query:
                context_parts.append(f"""
{country_name} êµ­ë°© ì •ë³´:
- êµ­ë°©ì˜ˆì‚°: {profile.defense_budget}
- ë³‘ë ¥ê·œëª¨: {profile.military_personnel}
- ì£¼ìš” ë°©ì‚°ê¸°ì—…: {', '.join(profile.defense_companies)}
- ì „ëµì  ì¤‘ìš”ë„: {profile.strategic_importance}
- í˜‘ë ¥ ìš©ì´ì„±: {profile.cooperation_feasibility}
""")
        
        return "\n".join(context_parts) if context_parts else "ì¼ë°˜ì ì¸ ë°©ì‚° í˜‘ë ¥ ì „ëµ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤."

    def _generate_knowledge_based_response(self, query: str, context: str) -> str:
        """ì§€ì‹ ë² ì´ìŠ¤ ê¸°ë°˜ ì‘ë‹µ ìƒì„±"""
        if "ìš°ì„ ìˆœìœ„" in query or "ìˆœìœ„" in query:
            return """ë°©ì‚° ìˆ˜ì¶œ ìš°ì„ ìˆœìœ„ëŠ” ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ê²°ì •ë©ë‹ˆë‹¤:

1. **ì‹œì¥ ê·œëª¨**: êµ­ë°©ì˜ˆì‚° ë° êµ¬ë§¤ë ¥
2. **ê¸°ìˆ  ìƒë³´ì„±**: ìƒí˜¸ ë³´ì™„ ê°€ëŠ¥í•œ ê¸°ìˆ  ì˜ì—­
3. **ì§€ì •í•™ì  ì¤‘ìš”ì„±**: ì „ëµì  íŒŒíŠ¸ë„ˆì‹­ ê°€ì¹˜
4. **í˜‘ë ¥ ìš©ì´ì„±**: ì •ì¹˜ì /ì œë„ì  ì¥ë²½ ìˆ˜ì¤€

êµ¬ì²´ì ì¸ êµ­ê°€ë³„ ìš°ì„ ìˆœìœ„ëŠ” ì§€ì—­ê³¼ ë¶„ì•¼ì— ë”°ë¼ ë‹¬ë¼ì§‘ë‹ˆë‹¤. íŠ¹ì • ì§€ì—­ì´ë‚˜ ê¸°ìˆ  ë¶„ì•¼ë¥¼ ëª…ì‹œí•´ ì£¼ì‹œë©´ ë” ìì„¸í•œ ì •ë³´ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."""

        elif "í˜‘ë ¥" in query and "ì „ëµ" in query:
            return """ë°©ì‚° ê¸°ìˆ  í˜‘ë ¥ ì „ëµì˜ í•µì‹¬ ìš”ì†Œë“¤:

### ğŸ“‹ í˜‘ë ¥ ëª¨ë¸
- **ê³µë™ê°œë°œ**: ìƒí˜¸ ê¸°ìˆ  ìœµí•©ì„ í†µí•œ ì‹ ì œí’ˆ ê°œë°œ
- **ê¸°ìˆ ì´ì „**: ë‹¨ê³„ì  ê¸°ìˆ  ì´ì „ì„ í†µí•œ í˜„ì§€ ìƒì‚°
- **íˆ¬ì í˜‘ë ¥**: í•©ì‘íˆ¬ìë¥¼ í†µí•œ ìƒì‚°ê¸°ì§€ êµ¬ì¶•

### ğŸ¯ ì„±ê³µ ìš”ì¸
- ìƒí˜¸ë³´ì™„ì  ê¸°ìˆ  ì—­ëŸ‰ í™•ì¸
- ì •ë¶€ ê°„ ì •ì±…ì  ì§€ì› í™•ë³´
- ì¥ê¸°ì  íŒŒíŠ¸ë„ˆì‹­ êµ¬ì¶•
- ì‹œì¥ ì§„ì¶œ ì „ëµ ìˆ˜ë¦½

ë” êµ¬ì²´ì ì¸ êµ­ê°€ë‚˜ ê¸°ìˆ  ë¶„ì•¼ì— ëŒ€í•œ ì§ˆë¬¸ì„ í•´ì£¼ì‹œë©´ ë§ì¶¤í˜• ì „ëµì„ ì œê³µí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."""

        else:
            return f"""ë°©ì‚° í˜‘ë ¥ ì „ëµ ê´€ë ¨ ì •ë³´:

{context}

### ğŸ’¡ ì£¼ìš” ê³ ë ¤ì‚¬í•­
- ê¸°ìˆ ì  ìƒë³´ì„± ë¶„ì„
- ì‹œì¥ ì§„ì… ì „ëµ ìˆ˜ë¦½
- ë¦¬ìŠ¤í¬ í‰ê°€ ë° ê´€ë¦¬
- ì¥ê¸°ì  íŒŒíŠ¸ë„ˆì‹­ êµ¬ì¶•

êµ¬ì²´ì ì¸ ì§ˆë¬¸ì´ë‚˜ íŠ¹ì • êµ­ê°€/ê¸°ìˆ  ë¶„ì•¼ì— ëŒ€í•´ ë¬¸ì˜í•´ ì£¼ì‹œë©´ ë” ìƒì„¸í•œ ì •ë³´ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."""

    def _generate_real_response(self, query: str, context: str) -> str:
        """ì‹¤ì œ ëª¨ë¸ì„ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„±"""
        try:
            prompt = f"""ë‹¤ìŒì€ ë°©ì‚° í˜‘ë ¥ ì „ëµì— ê´€í•œ ì§ˆë¬¸ì…ë‹ˆë‹¤.

ë°°ê²½ ì •ë³´: {context}

ì§ˆë¬¸: {query}

ë‹µë³€:"""
            
            inputs = self.tokenizer(
                prompt, 
                return_tensors="pt",
                max_length=512,
                truncation=True,
                padding=True
            )
            
            with torch.no_grad():
                if "t5" in self.config.model_name.lower():
                    outputs = self.model.generate(
                        input_ids=inputs['input_ids'],
                        attention_mask=inputs['attention_mask'],
                        max_new_tokens=self.config.max_tokens,
                        temperature=self.config.temperature,
                        do_sample=self.config.do_sample,
                        top_p=self.config.top_p,
                        pad_token_id=self.tokenizer.pad_token_id,
                        eos_token_id=self.tokenizer.eos_token_id
                    )
                    response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                else:
                    outputs = self.model.generate(
                        input_ids=inputs['input_ids'],
                        attention_mask=inputs['attention_mask'],
                        max_new_tokens=self.config.max_tokens,
                        temperature=self.config.temperature,
                        do_sample=self.config.do_sample,
                        top_p=self.config.top_p,
                        pad_token_id=self.tokenizer.pad_token_id,
                        eos_token_id=self.tokenizer.eos_token_id
                    )
                    response_tokens = outputs[0][inputs['input_ids'].shape[1]:]
                    response = self.tokenizer.decode(response_tokens, skip_special_tokens=True)
            
            return response.strip() if response.strip() else "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                
        except Exception as e:
            logger.error(f"Real model response failed: {e}")
            return self._generate_knowledge_based_response(query, context)

    def get_diversity_stats(self) -> Dict:
        """ë‹¤ì–‘ì„± í†µê³„ ì¡°íšŒ"""
        return self.diversity_manager.get_diversity_metrics()

    def reset_diversity_tracking(self):
        """ë‹¤ì–‘ì„± ì¶”ì  ì´ˆê¸°í™”"""
        self.diversity_manager.response_history = []
        self.diversity_manager.rejected_responses = []
        self.used_templates = []
        logger.info("Diversity tracking reset")
    
    def toggle_fallback_mode(self, enabled: bool = None) -> bool:
        """ìì²´ ë‹µë³€ ìƒì„± ëª¨ë“œ í† ê¸€"""
        if enabled is None:
            self.fallback_mode = not self.fallback_mode
        else:
            self.fallback_mode = enabled
        
        return self.fallback_mode
    
    def get_system_status(self) -> Dict:
        """ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ"""
        return {
            "fallback_mode": self.fallback_mode,
            "model_loaded": self.model is not None and self.model != "dummy_model",
            "knowledge_base_size": len(self.kb.countries) if hasattr(self.kb, 'countries') else 0,
            "response_templates": len(self.intelligent_generator.general_knowledge_templates)
        }

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    print("Enhanced Defense Cooperation LLM - 4ê°€ì§€ ì£¼ìš” ë¬¸ì œ í•´ê²° ì™„ë£Œ")
    print("1. T5 ëª¨ë¸ ì§€ì› âœ“")
    print("2. PDF ë°ì´í„° ê¸°ë°˜ ì •í™•í•œ ë‹µë³€ âœ“") 
    print("3. ì™„ì „í•œ ì‘ë‹µ ì¶œë ¥ âœ“")
    print("4. ë²”ìœ„ ì™¸ ì§ˆë¬¸ ì ì ˆí•œ ì²˜ë¦¬ âœ“")