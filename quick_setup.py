# quick_setup.py - ë¹ ë¥¸ ì„¤ì¹˜ ë° í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

import subprocess
import sys
import os

def install_packages():
    """í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜"""
    packages = [
        "torch",
        "transformers",
        "accelerate",
        "python-dotenv",
        "huggingface-hub"
    ]
    
    print("ğŸ“¦ í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘...")
    for package in packages:
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", package])
            print(f"âœ… {package} ì„¤ì¹˜ ì™„ë£Œ")
        except subprocess.CalledProcessError:
            print(f"âŒ {package} ì„¤ì¹˜ ì‹¤íŒ¨")

def test_basic_functionality():
    """ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ§ª ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 40)
    
    try:
        # ë°ì´í„° êµ¬ì¡° í…ŒìŠ¤íŠ¸
        print("1. ì§€ì‹ë² ì´ìŠ¤ í…ŒìŠ¤íŠ¸...")
        from src.data_structure import build_knowledge_base
        kb = build_knowledge_base()
        print(f"   âœ… ì§€ì‹ë² ì´ìŠ¤ êµ¬ì¶• ì„±ê³µ ({len(kb.countries)}ê°œêµ­)")
        
        # í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ í…ŒìŠ¤íŠ¸
        print("2. í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ í…ŒìŠ¤íŠ¸...")
        from src.prompt_engineering import create_comprehensive_prompt_system
        prompt_engineer = create_comprehensive_prompt_system(kb)
        print("   âœ… í”„ë¡¬í”„íŠ¸ ì‹œìŠ¤í…œ ìƒì„± ì„±ê³µ")
        
        # ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸
        test_prompt = prompt_engineer.generate_prompt("ì¸ë„ì™€ì˜ í˜‘ë ¥ ì „ëµì€?")
        print(f"   âœ… í”„ë¡¬í”„íŠ¸ ìƒì„± ì„±ê³µ ({len(test_prompt)} ë¬¸ì)")
        
        # AI ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ (ë”ë¯¸ ëª¨ë“œ)
        print("3. AI ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸...")
        from src.llama_integration import DefenseCooperationLlama, ModelConfig
        
        config = ModelConfig(
            model_name="dummy_model",  # ë”ë¯¸ ëª¨ë“œë¡œ ì‹œì‘
            use_quantization=False
        )
        
        llama_system = DefenseCooperationLlama(config, kb, prompt_engineer)
        llama_system._setup_dummy_mode()  # ë”ë¯¸ ëª¨ë“œ ê°•ì œ ì„¤ì •
        
        # í…ŒìŠ¤íŠ¸ ì‘ë‹µ ìƒì„±
        result = llama_system.generate_response("ì¸ë„ì™€ì˜ ë¯¸ì‚¬ì¼ ê¸°ìˆ  í˜‘ë ¥ ì „ëµì€?")
        if "error" not in result:
            print("   âœ… AI ì‘ë‹µ ìƒì„± ì„±ê³µ")
            print(f"   ğŸ“ ì‘ë‹µ ê¸¸ì´: {len(result['response'])} ë¬¸ì")
        else:
            print(f"   âŒ AI ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {result['response']}")
        
        print("\nğŸ‰ ëª¨ë“  ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ í†µê³¼!")
        return True
        
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

def run_interactive_demo():
    """ëŒ€í™”í˜• ë°ëª¨ ì‹¤í–‰"""
    print("\nğŸ¤– ëŒ€í™”í˜• ë°ëª¨ ì‹œì‘")
    print("=" * 40)
    
    try:
        from src.data_structure import build_knowledge_base
        from src.prompt_engineering import create_comprehensive_prompt_system
        from src.llama_integration import DefenseCooperationLlama, ModelConfig
        
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        kb = build_knowledge_base()
        prompt_engineer = create_comprehensive_prompt_system(kb)
        
        config = ModelConfig(model_name="dummy_model")
        llama_system = DefenseCooperationLlama(config, kb, prompt_engineer)
        llama_system._setup_dummy_mode()
        
        print("âœ… ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
        print("\nğŸ’¡ ì¶”ì²œ ì§ˆë¬¸:")
        print("  - ì¸ë„ì™€ì˜ ë¯¸ì‚¬ì¼ ê¸°ìˆ  í˜‘ë ¥ ì „ëµì€?")
        print("  - UAE íˆ¬ì ê·œëª¨ëŠ”?")
        print("  - ë¸Œë¼ì§ˆê³¼ í•­ê³µìš°ì£¼ í˜‘ë ¥ì´ ê°€ëŠ¥í•œê°€ìš”?")
        print("\nì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” ('ì¢…ë£Œ'ë¡œ ëë‚´ê¸°):")
        
        while True:
            user_input = input("\nğŸ‘¤ ì§ˆë¬¸: ").strip()
            
            if user_input.lower() in ['ì¢…ë£Œ', 'quit', 'exit', '']:
                print("ğŸ‘‹ ë°ëª¨ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤!")
                break
                
            if not user_input:
                continue
                
            print("ğŸ¤– AI: ", end="")
            result = llama_system.generate_response(user_input)
            
            if "error" not in result:
                print(result['response'])
                print(f"â± ìƒì„± ì‹œê°„: {result['generation_time']:.2f}ì´ˆ")
            else:
                print(f"ì˜¤ë¥˜: {result['response']}")
        
    except Exception as e:
        print(f"âŒ ë°ëª¨ ì‹¤í–‰ ì˜¤ë¥˜: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ë°©ì‚° í˜‘ë ¥ AI ì‹œìŠ¤í…œ ë¹ ë¥¸ ì„¤ì •")
    print("=" * 50)
    
    # ì‚¬ìš©ì ì„ íƒ
    print("ë‹¤ìŒ ì¤‘ ì„ íƒí•˜ì„¸ìš”:")
    print("1. íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° ê¸°ë³¸ í…ŒìŠ¤íŠ¸")
    print("2. ê¸°ë³¸ í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰")
    print("3. ëŒ€í™”í˜• ë°ëª¨ ì‹¤í–‰")
    print("4. ì „ì²´ ì‹¤í–‰ (ì„¤ì¹˜ â†’ í…ŒìŠ¤íŠ¸ â†’ ë°ëª¨)")
    
    choice = input("\nì„ íƒ (1-4): ").strip()
    
    if choice == "1":
        install_packages()
        if test_basic_functionality():
            print("\nâœ… ì„¤ì¹˜ ë° í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    elif choice == "2":
        if test_basic_functionality():
            print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    elif choice == "3":
        run_interactive_demo()
    elif choice == "4":
        install_packages()
        if test_basic_functionality():
            print("\nê³„ì†í•˜ë ¤ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")
            input()
            run_interactive_demo()
    else:
        print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")

if __name__ == "__main__":
    main()