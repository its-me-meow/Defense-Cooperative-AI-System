from src.data_structure import build_knowledge_base
from src.prompt_engineering import create_comprehensive_prompt_system
from src.llama_integration import DefenseCooperationLlama, ModelConfig
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

print("ğŸ§ª ë°©ì‚° í˜‘ë ¥ AI ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
print("=" * 50)

try:
    # ì§€ì‹ë² ì´ìŠ¤ êµ¬ì¶•
    print("ğŸ“š ì§€ì‹ë² ì´ìŠ¤ êµ¬ì¶• ì¤‘...")
    kb = build_knowledge_base()
    print(f"âœ… ì§€ì‹ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ ({len(kb.countries)}ê°œêµ­)")

    # í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ ìƒì„±
    print("ğŸ”§ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì‹œìŠ¤í…œ êµ¬ì¶• ì¤‘...")
    prompt_engineer = create_comprehensive_prompt_system(kb)
    print("âœ… í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì‹œìŠ¤í…œ êµ¬ì¶• ì™„ë£Œ")

    # ëª¨ë¸ ì„¤ì • (ì•ˆì •ì„± ìš°ì„ )
    config = ModelConfig(
        model_name="google/flan-t5-base",  # ë” ì•ˆì •ì ì¸ ëª¨ë¸
        max_tokens=1024,  # í† í° ê¸¸ì´ ì œí•œ
        temperature=0.8,
        use_quantization=False  # ì•ˆì •ì„±ì„ ìœ„í•´ ë¹„í™œì„±í™”
    )

    # Llama ì‹œìŠ¤í…œ ìƒì„±
    print("ğŸ¤– AI ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
    llama_system = DefenseCooperationLlama(config, kb, prompt_engineer)
    
    # ëª¨ë¸ ì´ˆê¸°í™” (ì˜¤ë¥˜ ë°œìƒ ì‹œ ìë™ìœ¼ë¡œ ë”ë¯¸ ëª¨ë“œë¡œ ì „í™˜)
    print("ğŸ“¥ ëª¨ë¸ ë¡œë”© ì¤‘... (ì²˜ìŒ ì‹¤í–‰ ì‹œ ë‹¤ìš´ë¡œë“œê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    llama_system.initialize_model()
    print("âœ… AI ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")

    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
    test_questions = [
        "ì¸ë„ì™€ì˜ ë¯¸ì‚¬ì¼ ê¸°ìˆ  í˜‘ë ¥ ì „ëµì€?",
        "UAE íˆ¬ì ê·œëª¨ëŠ” ì–´ëŠ ì •ë„ì¸ê°€ìš”?",
        "ë¸Œë¼ì§ˆê³¼ í•­ê³µìš°ì£¼ í˜‘ë ¥ì´ ê°€ëŠ¥í•œê°€ìš”?",
        "ë¹„NATO êµ­ê°€ ì¤‘ ìš°ì„  í˜‘ë ¥ ëŒ€ìƒêµ­ì€ ì–´ë””ì¸ê°€ìš”?"
    ]

    print("\nğŸš€ AI ì‘ë‹µ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 50)

    successful_tests = 0
    total_time = 0

    for i, question in enumerate(test_questions, 1):
        print(f"\nğŸ” í…ŒìŠ¤íŠ¸ {i}: {question}")
        print("-" * 60)
        
        try:
            result = llama_system.generate_response(question)
            
            if "error" not in result:
                print("âœ… ì‘ë‹µ ìƒì„± ì„±ê³µ")
                print(f"ğŸ¤– ëª¨ë“œ: {result['model_info']['mode']}")
                print(f"â± ìƒì„± ì‹œê°„: {result['generation_time']:.2f}ì´ˆ")
                print(f"ğŸ“ ì‘ë‹µ ê¸¸ì´: {result['response_length']} ë¬¸ì")
                
                # ì‘ë‹µ ë‚´ìš© (ì²˜ìŒ 400ìë§Œ ì¶œë ¥)
                response = result['response']
                if len(response) > 400:
                    print(f"ğŸ“„ ì‘ë‹µ ìƒ˜í”Œ:\n{response[:400]}...")
                else:
                    print(f"ğŸ“„ ì‘ë‹µ:\n{response}")
                
                successful_tests += 1
                total_time += result['generation_time']
                
            else:
                print(f"âŒ ì˜¤ë¥˜: {result.get('response', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                
        except Exception as e:
            print(f"âŒ ì˜ˆì™¸ ë°œìƒ: {e}")
            logger.error(f"Test {i} failed with exception: {e}")

    print("\n" + "="*60)
    print("ğŸ‰ ì „ì²´ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    
    if successful_tests > 0:
        print(f"âœ… ì„±ê³µí•œ í…ŒìŠ¤íŠ¸: {successful_tests}/{len(test_questions)}")
        print(f"ğŸ“Š í‰ê·  ì‘ë‹µ ì‹œê°„: {total_time/successful_tests:.2f}ì´ˆ")
        
        if successful_tests == len(test_questions):
            print("ğŸŠ ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        else:
            print("âš ï¸  ì¼ë¶€ í…ŒìŠ¤íŠ¸ëŠ” ì‹¤íŒ¨í–ˆì§€ë§Œ ê¸°ë³¸ ê¸°ëŠ¥ì€ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤")
    else:
        print("âŒ ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
        print("ğŸ’¡ í•´ê²°ë°©ë²•:")
        print("   1. ì¸í„°ë„· ì—°ê²° í™•ì¸")
        print("   2. pip install transformers torch --upgrade")
        print("   3. ê°€ìƒí™˜ê²½ ì¬í™œì„±í™”")
    
    # ì„±ëŠ¥ í†µê³„ ì¶œë ¥
    try:
        stats = llama_system.get_performance_stats()
        print(f"\nğŸ“ˆ ì‹œìŠ¤í…œ ì„±ëŠ¥ í†µê³„:")
        print(f"   - ì‚¬ìš© ëª¨ë¸: {stats['model_info']['model_name']}")
        if stats.get('total_conversations', 0) > 0:
            print(f"   - ì´ ëŒ€í™” ìˆ˜: {stats['total_conversations']}")
            print(f"   - í‰ê·  ìƒì„± ì‹œê°„: {stats['average_generation_time']:.2f}ì´ˆ")
    except Exception as e:
        logger.warning(f"Could not retrieve performance stats: {e}")

    print("\nğŸ’¡ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
    print("   â€¢ ë”ë¯¸ ëª¨ë“œë¡œë„ ì™„ì „í•œ ì „ë¬¸ ì‘ë‹µ ì œê³µ")
    print("   â€¢ ë‚˜ì¤‘ì— ì‹¤ì œ AI ëª¨ë¸ ì—°ê²° ê°€ëŠ¥")
    print("   â€¢ ëŒ€í™”í˜• ëª¨ë“œëŠ” quick_setup.py ì‹¤í–‰")

except Exception as e:
    print(f"\nâŒ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
    import traceback
    traceback.print_exc()
    
    print("\nğŸ”§ ë¬¸ì œ í•´ê²° ê°€ì´ë“œ:")
    print("1. ê°€ìƒí™˜ê²½ì´ ì œëŒ€ë¡œ í™œì„±í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸")
    print("   Windows: .venv\\Scripts\\activate")
    print("   Mac/Linux: source .venv/bin/activate")
    print("\n2. í•„ìˆ˜ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸")
    print("   pip install transformers torch python-dotenv")
    print("\n3. Python ê²½ë¡œ ë¬¸ì œì¼ ê²½ìš°")
    print("   python -c \"import sys; print(sys.path)\"")
    print("\n4. ê·¸ë˜ë„ ì•ˆ ë˜ë©´ ë” ì•ˆì „í•œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
    print("   python quick_setup.py")

finally:
    print(f"\nğŸ“‹ í…ŒìŠ¤íŠ¸ ì™„ë£Œ - {__file__}")